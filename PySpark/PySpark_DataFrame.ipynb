{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JuxtBHPuaaJ8",
        "eDEAYfpOgheA",
        "nGTRu73iuTMk",
        "Vzeg0KxYDsc9",
        "IP-8ghPhJnzI",
        "yXMypwI_dF4L",
        "JvBVDzimmbqU",
        "lei7eemaxlEU",
        "BhL_AFWZ2f12",
        "jRpVhXubT0sO",
        "yre1V7nxaxn_"
      ],
      "mount_file_id": "1tlaE1lpnh3TCh0Tr4_0Os-ttIL5OUizs",
      "authorship_tag": "ABX9TyNlrp+Rm4krofb72Q55CeEp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codal-tshah/data-practices-2024/blob/22_apr/PySpark/PySpark_DataFrame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH_I-Xy4ZSm0",
        "outputId": "ef2810c9-ec3e-4ba1-e102-019c529f2c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaJyttNvYADj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkExample.com').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create an empty DataFrame**"
      ],
      "metadata": {
        "id": "JuxtBHPuaaJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "print(emptyRDD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvhT5Ov4ZRJ2",
        "outputId": "1f6ef908-41ed-42c5-9a99-fc15407dc663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Empty DataFrame with Schema (StructType)**"
      ],
      "metadata": {
        "id": "843IRTKPfyXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "schema = StructType([\n",
        "  StructField('firstname', StringType(), True),\n",
        "  StructField('middlename', StringType(), True),\n",
        "  StructField('lastname', StringType(), True)\n",
        "  ])"
      ],
      "metadata": {
        "id": "fNOpqmmfaPzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(emptyRDD,schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7laxWdqf3Uq",
        "outputId": "18d4ad20-6741-4f0f-9f5c-785373d875e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n",
            "+---------+----------+--------+\n",
            "|firstname|middlename|lastname|\n",
            "+---------+----------+--------+\n",
            "+---------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Empty RDD to DataFrame**"
      ],
      "metadata": {
        "id": "pzNFpIlCf9ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.createDataFrame([],schema)\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AgL8VVQf6cN",
        "outputId": "1a08d7a3-e19e-4392-b298-a7b9832ee0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            "\n",
            "+---------+----------+--------+\n",
            "|firstname|middlename|lastname|\n",
            "+---------+----------+--------+\n",
            "+---------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert PySpark RDD to DataFrame**"
      ],
      "metadata": {
        "id": "eDEAYfpOgheA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create PySpark RDD**"
      ],
      "metadata": {
        "id": "bJg4riYxhJwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)"
      ],
      "metadata": {
        "id": "4X2E9Kp8hJNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJiUoBdggIOE",
        "outputId": "dfdfa41c-6f84-4074-c81a-a08381970d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlQBlD2xhsMc",
        "outputId": "91b21543-9b77-408e-8be7-88e6f1eb9151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inqd-I6oh9L5",
        "outputId": "bd171e8f-1ad9-4f8e-8fc0-7cf362ef7bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Completed Example**"
      ],
      "metadata": {
        "id": "Lh1CWW0SiVlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
        "rdd = spark.sparkContext.parallelize(dept)\n",
        "\n",
        "df = rdd.toDF()\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "df2 = rdd.toDF(deptColumns)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "deptSchema = StructType([\n",
        "    StructField('dept_name', StringType(), True),\n",
        "    StructField('dept_id', StringType(), True)\n",
        "])\n",
        "\n",
        "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
        "deptDF1.printSchema()\n",
        "deptDF1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teB2N1-riOl8",
        "outputId": "054cf078-9ea2-4f95-bbda-6568a7fd302b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+---------+---+\n",
            "|_1       |_2 |\n",
            "+---------+---+\n",
            "|Finance  |10 |\n",
            "|Marketing|20 |\n",
            "|Sales    |30 |\n",
            "|IT       |40 |\n",
            "+---------+---+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: string (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHOW()"
      ],
      "metadata": {
        "id": "ACshNuDFsRGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZt9Qh6nqjM1",
        "outputId": "bc02c946-d984-4760-b729-98f45ab6933c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|  Finance|     10|\n",
            "|Marketing|     20|\n",
            "|    Sales|     30|\n",
            "|       IT|     40|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show(2,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmlk2Cj6iYgo",
        "outputId": "d625a816-3d24-4aac-d5ed-3b9b5836ae5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "+---------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show(2,truncate=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03-oF8-Sqkrz",
        "outputId": "01627a49-74f5-4726-9b60-bb02e7c512fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|      Fin|     10|\n",
            "|      Mar|     20|\n",
            "+---------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show(2,truncate=False, vertical=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbPEpul8q3bT",
        "outputId": "fa24685a-e3ad-43bc-e276-4aaa279060a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0--------------\n",
            " dept_name | Finance   \n",
            " dept_id   | 10        \n",
            "-RECORD 1--------------\n",
            " dept_name | Marketing \n",
            " dept_id   | 20        \n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2.schema.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7h8IF-Fq97i",
        "outputId": "5f0e8df0-21c0-422f-b36e-fad87b27b4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"fields\":[{\"metadata\":{},\"name\":\"dept_name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"dept_id\",\"nullable\":true,\"type\":\"long\"}],\"type\":\"struct\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f6p0_f6ttxA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **StructType & StructField**"
      ],
      "metadata": {
        "id": "nGTRu73iuTMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType\n",
        "from pyspark.sql.functions import col,struct,when\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
        "  ]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"firstname\",StringType(),True),\n",
        "    StructField(\"middlename\",StringType(),True),\n",
        "    StructField(\"lastname\",StringType(),True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "  ])\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "structureData = [\n",
        "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
        "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
        "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
        "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
        "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
        "  ]\n",
        "structureSchema = StructType([\n",
        "        StructField('name', StructType([\n",
        "             StructField('firstname', StringType(), True),\n",
        "             StructField('middlename', StringType(), True),\n",
        "             StructField('lastname', StringType(), True)\n",
        "             ])),\n",
        "         StructField('id', StringType(), True),\n",
        "         StructField('gender', StringType(), True),\n",
        "         StructField('salary', IntegerType(), True)\n",
        "         ])\n",
        "\n",
        "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "\n",
        "updatedDF = df2.withColumn(\"OtherInfo\",\n",
        "    struct(col(\"id\").alias(\"identifier\"),\n",
        "    col(\"gender\").alias(\"gender\"),\n",
        "    col(\"salary\").alias(\"salary\"),\n",
        "    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
        "      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
        "      .otherwise(\"High\").alias(\"Salary_Grade\")\n",
        "  )).drop(\"id\",\"gender\",\"salary\")\n",
        "\n",
        "updatedDF.printSchema()\n",
        "updatedDF.show(truncate=False)\n",
        "\n",
        "\n",
        "\"\"\" Array & Map\"\"\"\n",
        "\n",
        "\n",
        "arrayStructureSchema = StructType([\n",
        "    StructField('name', StructType([\n",
        "       StructField('firstname', StringType(), True),\n",
        "       StructField('middlename', StringType(), True),\n",
        "       StructField('lastname', StringType(), True)\n",
        "       ])),\n",
        "       StructField('hobbies', ArrayType(StringType()), True),\n",
        "       StructField('properties', MapType(StringType(),StringType()), True)\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Qkm_RhuUal",
        "outputId": "54406c25-20d8-4e4f-9882-39925ba61af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|firstname|middlename|lastname|id   |gender|salary|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|James    |          |Smith   |36636|M     |3000  |\n",
            "|Michael  |Rose      |        |40288|M     |4000  |\n",
            "|Robert   |          |Williams|42114|M     |4000  |\n",
            "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
            "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
            "+---------+----------+--------+-----+------+------+\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+--------------------+-----+------+------+\n",
            "|name                |id   |gender|salary|\n",
            "+--------------------+-----+------+------+\n",
            "|{James, , Smith}    |36636|M     |3100  |\n",
            "|{Michael, Rose, }   |40288|M     |4300  |\n",
            "|{Robert, , Williams}|42114|M     |1400  |\n",
            "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
            "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
            "+--------------------+-----+------+------+\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- OtherInfo: struct (nullable = false)\n",
            " |    |-- identifier: string (nullable = true)\n",
            " |    |-- gender: string (nullable = true)\n",
            " |    |-- salary: integer (nullable = true)\n",
            " |    |-- Salary_Grade: string (nullable = false)\n",
            "\n",
            "+--------------------+------------------------+\n",
            "|name                |OtherInfo               |\n",
            "+--------------------+------------------------+\n",
            "|{James, , Smith}    |{36636, M, 3100, Medium}|\n",
            "|{Michael, Rose, }   |{40288, M, 4300, High}  |\n",
            "|{Robert, , Williams}|{42114, M, 1400, Low}   |\n",
            "|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n",
            "|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n",
            "+--------------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Column Class | Operators & Functions**"
      ],
      "metadata": {
        "id": "Vzeg0KxYDsc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Column Class Object**"
      ],
      "metadata": {
        "id": "qDDCEqc7F75k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "colObj = lit(\"sparkbyexamples.com\")"
      ],
      "metadata": {
        "id": "AYSvSb8SuUzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"James\",23),(\"Ann\",40)]\n",
        "df=spark.createDataFrame(data).toDF(\"name\",\"gender\")\n",
        "df.printSchema()\n",
        "\n",
        "# Using DataFrame object (df)\n",
        "df.select(df.gender).show()\n",
        "df.select(df[\"gender\"]).show()\n",
        "#Accessing column name with dot (with backticks)\n",
        "df.select(df[\"`name`\"]).show()\n",
        "\n",
        "#Using SQL col() function\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"gender\")).show()\n",
        "#Accessing column name with dot (with backticks)\n",
        "df.select(col(\"`name`\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOiGveIZv6-H",
        "outputId": "1fff5751-8535-4f7c-d6a3-5dfa4c16a2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: long (nullable = true)\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|    23|\n",
            "|    40|\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|    23|\n",
            "|    40|\n",
            "+------+\n",
            "\n",
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "|James|\n",
            "|  Ann|\n",
            "+-----+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|    23|\n",
            "|    40|\n",
            "+------+\n",
            "\n",
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "|James|\n",
            "|  Ann|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')]\n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0gJDTFaIXX6",
        "outputId": "64618ab8-8324-4901-ae77-e4121d22fc19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|     fname|lname| id|gender|\n",
            "+----------+-----+---+------+\n",
            "|     James| Bond|100|  NULL|\n",
            "|       Ann|Varsa|200|     F|\n",
            "|Tom Cruise|  XXX|400|      |\n",
            "| Tom Brand| NULL|400|     M|\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#alias\n",
        "from pyspark.sql.functions import expr\n",
        "df.select(df.fname.alias(\"first_name\"),\n",
        "          df.lname.alias(\"last_name\")\n",
        "   ).show()\n",
        "\n",
        "#Another example\n",
        "df.select(expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n",
        "   ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIlRqfY9GJxy",
        "outputId": "f8d533d1-fec6-4424-91f0-3f950bff2e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|first_name|last_name|\n",
            "+----------+---------+\n",
            "|     James|     Bond|\n",
            "|       Ann|    Varsa|\n",
            "|Tom Cruise|      XXX|\n",
            "| Tom Brand|     NULL|\n",
            "+----------+---------+\n",
            "\n",
            "+--------------+\n",
            "|      fullName|\n",
            "+--------------+\n",
            "|    James,Bond|\n",
            "|     Ann,Varsa|\n",
            "|Tom Cruise,XXX|\n",
            "|          NULL|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(df.fname.asc()).show()\n",
        "df.sort(df.fname.desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r_lLv46H-Nv",
        "outputId": "968fe884-6b8c-4148-b1c9-7f79e1b331dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---+------+\n",
            "|     fname|lname| id|gender|\n",
            "+----------+-----+---+------+\n",
            "|       Ann|Varsa|200|     F|\n",
            "|     James| Bond|100|  NULL|\n",
            "| Tom Brand| NULL|400|     M|\n",
            "|Tom Cruise|  XXX|400|      |\n",
            "+----------+-----+---+------+\n",
            "\n",
            "+----------+-----+---+------+\n",
            "|     fname|lname| id|gender|\n",
            "+----------+-----+---+------+\n",
            "|Tom Cruise|  XXX|400|      |\n",
            "| Tom Brand| NULL|400|     M|\n",
            "|     James| Bond|100|  NULL|\n",
            "|       Ann|Varsa|200|     F|\n",
            "+----------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.id.between(100,300)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFQ4wliAIvZS",
        "outputId": "ecaba42d-bfd2-44dd-f5ba-81175e7da898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+---+------+\n",
            "|fname|lname| id|gender|\n",
            "+-----+-----+---+------+\n",
            "|James| Bond|100|  NULL|\n",
            "|  Ann|Varsa|200|     F|\n",
            "+-----+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.fname.contains(\"B\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJjLUZe4JEb3",
        "outputId": "73795711-790d-4e44-e2da-823cb24cd86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+---+------+\n",
            "|    fname|lname| id|gender|\n",
            "+---------+-----+---+------+\n",
            "|Tom Brand| NULL|400|     M|\n",
            "+---------+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.fname,df.lname,df.id) \\\n",
        "  .filter(df.fname.like(\"%om\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMHOva_1JV07",
        "outputId": "daa1ea04-a1a8-482f-dc31-34c49d4028c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[fname: string, lname: string, id: string]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
        "              .when(df.gender==\"F\",\"Female\") \\\n",
        "              .when(df.gender==None ,\"\") \\\n",
        "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
        "    ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KajYhmW_JxHp",
        "outputId": "ceb3f545-0db4-4e9d-e380-fe546dbf77c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+\n",
            "|     fname|lname|new_gender|\n",
            "+----------+-----+----------+\n",
            "|     James| Bond|      NULL|\n",
            "|       Ann|Varsa|    Female|\n",
            "|Tom Cruise|  XXX|          |\n",
            "| Tom Brand| NULL|      Male|\n",
            "+----------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.gender.isNull()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNvz2seFJ9bc",
        "outputId": "576b1291-1ace-4ee3-ff42-5090dcb3a07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+---+------+\n",
            "|fname|lname| id|gender|\n",
            "+-----+-----+---+------+\n",
            "|James| Bond|100|  NULL|\n",
            "+-----+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "z5abJVCDKl_h",
        "outputId": "8a4f9749-c8b0-4c90-bca8-9a49ebf6fb44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- fname: string (nullable = true)\n",
            " |-- lname: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getItem() used with ArrayType\n",
        "# df.select(df.languages.getItem(1)).show()\n",
        "\n",
        "#getItem() used with MapType\n",
        "df.select(df.lname.getItem(\"xxx\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "o6OxK9wFKPca",
        "outputId": "ec541139-6f39-4fc5-b151-7b1aa62e0a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"lname\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\".",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a79a51190a54>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#getItem() used with MapType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xxx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m         \"\"\"\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"lname\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\"."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-03haz4YKbJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Select Columns**"
      ],
      "metadata": {
        "id": "IP-8ghPhJnzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark.examples.com\").getOrCreate()\n",
        "data = [(\"mahesh ihfweoifow\", \"sci\", 15, \"ssoa\"),\n",
        "    (\"ramesh\", \"comm\", 17, \"xaviers\"),\n",
        "    (\"fenil a;h;hwo;eihfwoeih\", \"arts\", 16, \"dhl\"),\n",
        "    (\"kenil\", \"sci\", 18, \"infocity\")\n",
        "]\n",
        "columns = [\"Name\", \"stream\", \"Age\", \"School\"]\n",
        "df = spark.createDataFrame(data = data, schema=columns)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSpd9BMlJsf1",
        "outputId": "3e05a57e-3018-4800-9ac7-89f93e154ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+---+--------+\n",
            "|                Name|stream|Age|  School|\n",
            "+--------------------+------+---+--------+\n",
            "|   mahesh ihfweoifow|   sci| 15|    ssoa|\n",
            "|              ramesh|  comm| 17| xaviers|\n",
            "|fenil a;h;hwo;eih...|  arts| 16|     dhl|\n",
            "|               kenil|   sci| 18|infocity|\n",
            "+--------------------+------+---+--------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- stream: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- School: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "880fgJLSK7Yw",
        "outputId": "88bed695-0833-4e1b-c543-4f67eaeb37bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"firstname\",\"lastname\").show()\n",
        "df.select(df.firstname,df.lastname).show()\n",
        "df.select(df[\"firstname\"],df[\"lastname\"]).show()\n",
        "\n",
        "#By using col() function\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
        "\n",
        "#Select columns by regular expression\n",
        "df.select(df.colRegex(\"`^.*name*`\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qX8JeVELyMe",
        "outputId": "c264ab47-7731-4e3e-b234-08d7eee8e85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    James|   Smith|\n",
            "|  Michael|    Rose|\n",
            "|   Robert|Williams|\n",
            "|    Maria|   Jones|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    James|   Smith|\n",
            "|  Michael|    Rose|\n",
            "|   Robert|Williams|\n",
            "|    Maria|   Jones|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    James|   Smith|\n",
            "|  Michael|    Rose|\n",
            "|   Robert|Williams|\n",
            "|    Maria|   Jones|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    James|   Smith|\n",
            "|  Michael|    Rose|\n",
            "|   Robert|Williams|\n",
            "|    Maria|   Jones|\n",
            "+---------+--------+\n",
            "\n",
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    James|   Smith|\n",
            "|  Michael|    Rose|\n",
            "|   Robert|Williams|\n",
            "|    Maria|   Jones|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(*columns).show()\n",
        "\n",
        "# Select All columns\n",
        "df.select([col for col in df.columns]).show()\n",
        "df.select(\"*\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLwS5NlsL3ff",
        "outputId": "f153acb5-2911-40ad-b01b-06de8949c2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|    James|   Smith|    USA|   CA|\n",
            "|  Michael|    Rose|    USA|   NY|\n",
            "|   Robert|Williams|    USA|   CA|\n",
            "|    Maria|   Jones|    USA|   FL|\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|    James|   Smith|    USA|   CA|\n",
            "|  Michael|    Rose|    USA|   NY|\n",
            "|   Robert|Williams|    USA|   CA|\n",
            "|    Maria|   Jones|    USA|   FL|\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|    James|   Smith|    USA|   CA|\n",
            "|  Michael|    Rose|    USA|   NY|\n",
            "|   Robert|Williams|    USA|   CA|\n",
            "|    Maria|   Jones|    USA|   FL|\n",
            "+---------+--------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selects first 3 columns and top 3 rows\n",
        "df.select(df.columns[:3]).show(3)\n",
        "\n",
        "#Selects columns 2 to 4  and top 3 r\n",
        "df.select(df.columns[2:4]).show(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ0qfBgsMBXp",
        "outputId": "9ea1d88e-7c3f-4193-8ade-36c8eeb3f357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+\n",
            "|firstname|lastname|country|\n",
            "+---------+--------+-------+\n",
            "|    James|   Smith|    USA|\n",
            "|  Michael|    Rose|    USA|\n",
            "|   Robert|Williams|    USA|\n",
            "+---------+--------+-------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-------+-----+\n",
            "|country|state|\n",
            "+-------+-----+\n",
            "|    USA|   CA|\n",
            "|    USA|   NY|\n",
            "|    USA|   CA|\n",
            "+-------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUwSi2-sc_mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COLLECT()**"
      ],
      "metadata": {
        "id": "yXMypwI_dF4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver.**"
      ],
      "metadata": {
        "id": "S9ELKAwTdg0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxmTZnS1dLH1",
        "outputId": "3018b556-243e-4cc4-c52d-85120bddc96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataCollect = deptDF.collect()\n",
        "print(dataCollect)"
      ],
      "metadata": {
        "id": "poSuA0xMMESg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd0340a-4f1f-4473-b3b9-c41f2ec9d860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row in dataCollect:\n",
        "    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMygVA38dWgF",
        "outputId": "a11fdc7a-ace7-4a87-d22e-fffc0301cc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finance,10\n",
            "Marketing,20\n",
            "Sales,30\n",
            "IT,40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Where Filter**"
      ],
      "metadata": {
        "id": "JvBVDzimmbqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField\n",
        "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
        "data = [\n",
        "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
        "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
        "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
        "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
        "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
        "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
        " ]\n",
        "\n",
        "schema = StructType([\n",
        "     StructField('name', StructType([\n",
        "        StructField('firstname', StringType(), True),\n",
        "        StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "     ])),\n",
        "     StructField('languages', ArrayType(StringType()), True),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        " ])\n",
        "\n",
        "df = spark.createDataFrame(data = data, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl-OtFQ5lMLN",
        "outputId": "87bcd960-7d10-4c05-fed4-d5b410815b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- languages: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.state == \"OH\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU0Fp_MemiDE",
        "outputId": "70eaafbb-6261-4c7f-b7b3-de4049d8af81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+------------------+-----+------+\n",
            "|name                  |languages         |state|gender|\n",
            "+----------------------+------------------+-----+------+\n",
            "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
            "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
            "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
            "+----------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not equals condition\n",
        "df.filter(df.state != \"OH\") \\\n",
        "    .show(truncate=False)\n",
        "df.filter(~(df.state == \"OH\")) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQQL1H6_mwNS",
        "outputId": "0c19587c-c345-4e2d-d20a-14aaccfb9368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|name                |languages         |state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
            "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
            "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
            "+--------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\"gender <> 'M'\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaorUx1-myWa",
        "outputId": "83f64d87-0780-4ca8-ab84-166f75fcca2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+-----+------+\n",
            "|               name|         languages|state|gender|\n",
            "+-------------------+------------------+-----+------+\n",
            "|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
            "|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
            "+-------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "li=[\"OH\",\"CA\",\"DE\"]\n",
        "df.filter(df.state.isin(li)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls0Z07Hsm6Zh",
        "outputId": "363126ab-75a3-4328-f649-e31be5211368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+-----+------+\n",
            "|                name|         languages|state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
            "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
            "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
            "+--------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter NOT IS IN List values\n",
        "#These show all records with NY (NY is not part of the list)\n",
        "df.filter(~df.state.isin(li)).show()\n",
        "df.filter(df.state.isin(li)==False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuDvrSwZnC9E",
        "outputId": "73682063-4843-487b-8896-432500dd957c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+-----+------+\n",
            "|                name|         languages|state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
            "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
            "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
            "+--------------------+------------------+-----+------+\n",
            "\n",
            "+--------------------+------------------+-----+------+\n",
            "|                name|         languages|state|gender|\n",
            "+--------------------+------------------+-----+------+\n",
            "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
            "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
            "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
            "+--------------------+------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare Data\n",
        "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n",
        "     (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
        "  ]\n",
        "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
        "\n",
        "# like - SQL LIKE pattern\n",
        "df2.filter(df2.name.like(\"%rose%\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmJYGEoenNEH",
        "outputId": "57f9c91c-c388-4339-a14d-49e7c8e4c7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id|      name|\n",
            "+---+----------+\n",
            "|  5|Rames rose|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPpoZU2wnsJS",
        "outputId": "10656a5c-3160-4868-afa5-05cf2edc4bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+\n",
            "| id|        name|\n",
            "+---+------------+\n",
            "|  2|Michael Rose|\n",
            "|  4|  Rames Rose|\n",
            "|  5|  Rames rose|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distinct to Drop Duplicate**"
      ],
      "metadata": {
        "id": "lei7eemaxlEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "distinct() transformation is used to drop/remove the duplicate rows (all columns) from DataFrame and dropDuplicates() is used to drop rows based on selected (one or multiple) columns."
      ],
      "metadata": {
        "id": "76WEL2SLxicJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "data = [(\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600), \\\n",
        "    (\"Robert\", \"Sales\", 4100), \\\n",
        "    (\"Maria\", \"Finance\", 3000), \\\n",
        "    (\"James\", \"Sales\", 3000), \\\n",
        "    (\"Scott\", \"Finance\", 3300), \\\n",
        "    (\"Jen\", \"Finance\", 3900), \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000), \\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  ]\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "#Distinct\n",
        "distinctDF = df.distinct()\n",
        "print(\"Distinct count: \"+str(distinctDF.count()))\n",
        "distinctDF.show(truncate=False)\n",
        "\n",
        "#Drop duplicates\n",
        "df2 = df.dropDuplicates()\n",
        "print(\"Distinct count: \"+str(df2.count()))\n",
        "df2.show(truncate=False)\n",
        "\n",
        "#Drop duplicates on selected columns\n",
        "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
        "print(\"Distinct count of department salary : \"+str(dropDisDF.count()))\n",
        "dropDisDF.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A317aTlNnuQB",
        "outputId": "4697d4b7-f346-408b-e919-c7e4cd7d049b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "Distinct count: 9\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Michael      |Sales     |4600  |\n",
            "|James        |Sales     |3000  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "Distinct count: 9\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Michael      |Sales     |4600  |\n",
            "|James        |Sales     |3000  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "Distinct count of department salary : 8\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Maria        |Finance   |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Michael      |Sales     |4600  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAP()**"
      ],
      "metadata": {
        "id": "BhL_AFWZ2f12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "data = [\"Project\",\n",
        "\"Gutenberg’s\",\n",
        "\"Alice’s\",\n",
        "\"Adventures\",\n",
        "\"in\",\n",
        "\"Wonderland\",\n",
        "\"Project\",\n",
        "\"Gutenberg’s\",\n",
        "\"Adventures\",\n",
        "\"in\",\n",
        "\"Wonderland\",\n",
        "\"Project\",\n",
        "\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd2=rdd.map(lambda x: (x,1))\n",
        "for element in rdd2.collect():\n",
        "    print(element)\n",
        "\n",
        "data = [('James','Smith','M',30),\n",
        "  ('Anna','Rose','F',41),\n",
        "  ('Robert','Williams','M',62),\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.show()\n",
        "\n",
        "rdd2=df.rdd.map(lambda x:\n",
        "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
        "    )\n",
        "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\n",
        "df2.show()\n",
        "\n",
        "#Referring Column Names\n",
        "rdd2=df.rdd.map(lambda x:\n",
        "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
        "    )\n",
        "\n",
        "#Referring Column Names\n",
        "rdd2=df.rdd.map(lambda x:\n",
        "    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n",
        "    )\n",
        "\n",
        "def func1(x):\n",
        "    firstName=x.firstname\n",
        "    lastName=x.lastname\n",
        "    name=firstName+\",\"+lastName\n",
        "    gender=x.gender.lower()\n",
        "    salary=x.salary*2\n",
        "    return (name,gender,salary)\n",
        "\n",
        "rdd2=df.rdd.map(lambda x: func1(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bvIaadnxx73",
        "outputId": "43de2a8f-e164-4698-d694-6b6b1d871ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Project', 1)\n",
            "('Gutenberg’s', 1)\n",
            "('Alice’s', 1)\n",
            "('Adventures', 1)\n",
            "('in', 1)\n",
            "('Wonderland', 1)\n",
            "('Project', 1)\n",
            "('Gutenberg’s', 1)\n",
            "('Adventures', 1)\n",
            "('in', 1)\n",
            "('Wonderland', 1)\n",
            "('Project', 1)\n",
            "('Gutenberg’s', 1)\n",
            "+---------+--------+------+------+\n",
            "|firstname|lastname|gender|salary|\n",
            "+---------+--------+------+------+\n",
            "|    James|   Smith|     M|    30|\n",
            "|     Anna|    Rose|     F|    41|\n",
            "|   Robert|Williams|     M|    62|\n",
            "+---------+--------+------+------+\n",
            "\n",
            "+---------------+------+----------+\n",
            "|           name|gender|new_salary|\n",
            "+---------------+------+----------+\n",
            "|    James,Smith|     M|        60|\n",
            "|      Anna,Rose|     F|        82|\n",
            "|Robert,Williams|     M|       124|\n",
            "+---------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FLAT MAP()**"
      ],
      "metadata": {
        "id": "HGLlGXma49QL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "data = [\"Project Gutenberg’s\",\n",
        "        \"Alice’s Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\",\n",
        "        \"Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\"]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "for element in rdd.collect():\n",
        "    print(element)\n",
        "\n",
        "#Flatmap\n",
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "for element in rdd2.collect():\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJy6XD_B2jRK",
        "outputId": "373f710c-b638-4ba3-ca21-37d8a3b7c8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Gutenberg’s\n",
            "Alice’s Adventures in Wonderland\n",
            "Project Gutenberg’s\n",
            "Adventures in Wonderland\n",
            "Project Gutenberg’s\n",
            "Project\n",
            "Gutenberg’s\n",
            "Alice’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n",
            "Adventures\n",
            "in\n",
            "Wonderland\n",
            "Project\n",
            "Gutenberg’s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unfortunately, PySpark DataFame doesn’t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column.**"
      ],
      "metadata": {
        "id": "sJ0bFne-5gLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
        "\n",
        "arrayData = [\n",
        "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
        "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
        "        ('Washington',None,None),\n",
        "        ('Jefferson',['1','2'],{})]\n",
        "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "df2 = df.select(df.name,explode(df.knownLanguages))\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqVUiNj35Adr",
        "outputId": "2dc07e9b-27b7-4bee-e164-0160da087d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- col: string (nullable = true)\n",
            "\n",
            "+---------+------+\n",
            "|     name|   col|\n",
            "+---------+------+\n",
            "|    James|  Java|\n",
            "|    James| Scala|\n",
            "|  Michael| Spark|\n",
            "|  Michael|  Java|\n",
            "|  Michael|  NULL|\n",
            "|   Robert|CSharp|\n",
            "|   Robert|      |\n",
            "|Jefferson|     1|\n",
            "|Jefferson|     2|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df=spark.range(100)\n",
        "print(df.sample(0.05,123).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27UNq-cl5EiK",
        "outputId": "062d638c-99ce-46b5-9ae1-53d052750b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(id=36), Row(id=75), Row(id=97)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8Wrj4_URjp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **orderBy() and sort()**"
      ],
      "metadata": {
        "id": "jRpVhXubT0sO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, asc,desc\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "df.sort(\"department\",\"state\").show(truncate=False)\n",
        "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
        "\n",
        "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
        "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
        "\n",
        "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
        "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
        "df.orderBy(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
        "\n",
        "df.sort(df.department.asc(),df.state.desc()).show(truncate=False)\n",
        "df.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
        "df.orderBy(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)"
      ],
      "metadata": {
        "id": "45eDBACt8MD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7d1fb6-83c5-4b1f-f6e8-8a6954051989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Union and UnionAll**"
      ],
      "metadata": {
        "id": "yre1V7nxaxn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
        "  ]\n",
        "\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
        "\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "unionDF = df.union(df2)\n",
        "unionDF.show(truncate=False)\n",
        "disDF = df.union(df2).distinct()\n",
        "disDF.show(truncate=False)\n",
        "\n",
        "unionAllDF = df.unionAll(df2)\n",
        "unionAllDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3X_DaA-TGK6",
        "outputId": "89f5c47a-dc7e-4185-d022-b0ec5b120852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformation**"
      ],
      "metadata": {
        "id": "Lcwx8UA9fCA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pyspark.sql.DataFrame.transform() – Available since Spark 3.0\n",
        "- pyspark.sql.functions.transform()\n"
      ],
      "metadata": {
        "id": "fUDG0Oz1lKMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .appName('SparkByExamples.com') \\\n",
        "            .getOrCreate()\n",
        "\n",
        "# Prepare Data\n",
        "simpleData = ((\"Java\",4000,5), \\\n",
        "    (\"Python\", 4600,10),  \\\n",
        "    (\"Scala\", 4100,15),   \\\n",
        "    (\"Scala\", 4500,15),   \\\n",
        "    (\"PHP\", 3000,20),  \\\n",
        "  )\n",
        "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV_9Z4nhnzp4",
        "outputId": "89b18b18-242e-4f2f-f60d-78a68e8a8fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CourseName: string (nullable = true)\n",
            " |-- fee: long (nullable = true)\n",
            " |-- discount: long (nullable = true)\n",
            "\n",
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "|Scala     |4500|15      |\n",
            "|PHP       |3000|20      |\n",
            "+----------+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "def to_upper_str_columns(df):\n",
        "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
        "\n",
        "# Custom transformation 2\n",
        "def reduce_price(df,reduceBy):\n",
        "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
        "\n",
        "# Custom transformation 3\n",
        "def apply_discount(df):\n",
        "    return df.withColumn(\"discounted_fee\",  \\\n",
        "             df.new_fee - (df.new_fee * df.discount) / 100)\n"
      ],
      "metadata": {
        "id": "Gi8whyJXospn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.transform(to_upper_str_columns) \\\n",
        "        .transform(reduce_price,1000) \\\n",
        "        .transform(apply_discount)\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSGSqGxypln7",
        "outputId": "bd3ea391-6950-422f-c158-3f1b42603d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+--------+-------+--------------+\n",
            "|CourseName| fee|discount|new_fee|discounted_fee|\n",
            "+----------+----+--------+-------+--------------+\n",
            "|      JAVA|4000|       5|   3000|        2850.0|\n",
            "|    PYTHON|4600|      10|   3600|        3240.0|\n",
            "|     SCALA|4100|      15|   3100|        2635.0|\n",
            "|     SCALA|4500|      15|   3500|        2975.0|\n",
            "|       PHP|3000|      20|   2000|        1600.0|\n",
            "+----------+----+--------+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .appName('SparkByExamples.com') \\\n",
        "            .getOrCreate()\n",
        "\n",
        "# Prepare Data\n",
        "simpleData = ((\"Java\",4000,5), \\\n",
        "    (\"Python\", 4600,10),  \\\n",
        "    (\"Scala\", 4100,15),   \\\n",
        "    (\"Scala\", 4500,15),   \\\n",
        "    (\"PHP\", 3000,20),  \\\n",
        "  )\n",
        "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Custom transformation 1\n",
        "from pyspark.sql.functions import upper\n",
        "def to_upper_str_columns(df):\n",
        "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
        "\n",
        "# Custom transformation 2\n",
        "def reduce_price(df,reduceBy):\n",
        "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
        "\n",
        "# Custom transformation 3\n",
        "def apply_discount(df):\n",
        "    return df.withColumn(\"discounted_fee\",  \\\n",
        "             df.new_fee - (df.new_fee * df.discount) / 100)\n",
        "\n",
        "# transform() usage\n",
        "df2 = df.transform(to_upper_str_columns) \\\n",
        "        .transform(reduce_price,1000) \\\n",
        "        .transform(apply_discount)\n",
        "\n",
        "df2.show()\n",
        "\n",
        "# Create DataFrame with Array\n",
        "data = [\n",
        " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
        " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
        " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
        "]\n",
        "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
        "df.printSchema()\n",
        "df.show()\n",
        "\n",
        "# using transform() SQL function\n",
        "from pyspark.sql.functions import upper\n",
        "from pyspark.sql.functions import transform\n",
        "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")) \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU4IMr_HaBak",
        "outputId": "849185c5-13fe-4e80-fcd4-78494219b73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CourseName: string (nullable = true)\n",
            " |-- fee: long (nullable = true)\n",
            " |-- discount: long (nullable = true)\n",
            "\n",
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "|Scala     |4500|15      |\n",
            "|PHP       |3000|20      |\n",
            "+----------+----+--------+\n",
            "\n",
            "+----------+----+--------+-------+--------------+\n",
            "|CourseName| fee|discount|new_fee|discounted_fee|\n",
            "+----------+----+--------+-------+--------------+\n",
            "|      JAVA|4000|       5|   3000|        2850.0|\n",
            "|    PYTHON|4600|      10|   3600|        3240.0|\n",
            "|     SCALA|4100|      15|   3100|        2635.0|\n",
            "|     SCALA|4500|      15|   3500|        2975.0|\n",
            "|       PHP|3000|      20|   2000|        1600.0|\n",
            "+----------+----+--------+-------+--------------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Languages1: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- Languages2: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------+------------------+---------------+\n",
            "|            Name|        Languages1|     Languages2|\n",
            "+----------------+------------------+---------------+\n",
            "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
            "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
            "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
            "+----------------+------------------+---------------+\n",
            "\n",
            "+------------------+\n",
            "|        languages1|\n",
            "+------------------+\n",
            "|[JAVA, SCALA, C++]|\n",
            "|[SPARK, JAVA, C++]|\n",
            "|      [CSHARP, VB]|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **fillna() & fill()**"
      ],
      "metadata": {
        "id": "PPoR1mOPq_hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SparkSession and read csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "filePath=\"/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2.csv\"\n",
        "df = spark.read.options(header='true', inferSchema='true') \\\n",
        "          .csv(filePath)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMNlxvDDfCsx",
        "outputId": "b57d2b90-ea81-4721-e6c6-df62978fbc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- datetime: timestamp (nullable = true)\n",
            " |-- station_antwerp: double (nullable = true)\n",
            " |-- station_paris: double (nullable = true)\n",
            " |-- station_london: double (nullable = true)\n",
            "\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|datetime           |station_antwerp|station_paris|station_london|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|2019-05-07 02:00:00|NULL           |NULL         |23.0          |\n",
            "|2019-05-07 03:00:00|50.5           |25.0         |19.0          |\n",
            "|2019-05-07 04:00:00|45.0           |27.7         |19.0          |\n",
            "|2019-05-07 05:00:00|NULL           |50.4         |16.0          |\n",
            "|2019-05-07 06:00:00|NULL           |61.9         |NULL          |\n",
            "|2019-05-07 07:00:00|NULL           |72.4         |26.0          |\n",
            "|2019-05-07 08:00:00|NULL           |77.7         |32.0          |\n",
            "|2019-05-07 09:00:00|NULL           |67.9         |32.0          |\n",
            "|2019-05-07 10:00:00|NULL           |56.0         |28.0          |\n",
            "|2019-05-07 11:00:00|NULL           |34.5         |21.0          |\n",
            "|2019-05-07 12:00:00|NULL           |20.1         |21.0          |\n",
            "|2019-05-07 13:00:00|NULL           |13.0         |18.0          |\n",
            "|2019-05-07 14:00:00|NULL           |10.6         |20.0          |\n",
            "|2019-05-07 15:00:00|NULL           |13.2         |18.0          |\n",
            "|2019-05-07 16:00:00|NULL           |11.0         |20.0          |\n",
            "|2019-05-07 17:00:00|NULL           |11.7         |20.0          |\n",
            "|2019-05-07 18:00:00|NULL           |18.2         |21.0          |\n",
            "|2019-05-07 19:00:00|NULL           |22.3         |20.0          |\n",
            "|2019-05-07 20:00:00|NULL           |21.4         |20.0          |\n",
            "|2019-05-07 21:00:00|NULL           |26.8         |24.0          |\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "df.filter(df.station_paris.isNull()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_U8YbBhsn2F",
        "outputId": "49933842-b2e0-4401-de63-1938d860894d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------+-------------+--------------+\n",
            "|           datetime|station_antwerp|station_paris|station_london|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|2019-05-07 02:00:00|           NULL|         NULL|          23.0|\n",
            "|2019-05-11 05:00:00|           NULL|         NULL|          35.0|\n",
            "|2019-05-11 06:00:00|           NULL|         NULL|          35.0|\n",
            "|2019-05-11 07:00:00|           NULL|         NULL|          30.0|\n",
            "|2019-05-16 08:00:00|           NULL|         NULL|          34.0|\n",
            "|2019-05-25 05:00:00|           NULL|         NULL|          21.0|\n",
            "|2019-05-25 06:00:00|           NULL|         NULL|          21.0|\n",
            "|2019-05-25 07:00:00|           NULL|         NULL|          22.0|\n",
            "|2019-06-01 05:00:00|           NULL|         NULL|          11.0|\n",
            "|2019-06-01 06:00:00|           NULL|         NULL|          11.0|\n",
            "|2019-06-01 07:00:00|           NULL|         NULL|           4.0|\n",
            "|2019-06-06 17:00:00|           NULL|         NULL|          22.0|\n",
            "|2019-06-06 18:00:00|           NULL|         NULL|          24.0|\n",
            "|2019-06-06 19:00:00|           NULL|         NULL|          24.0|\n",
            "|2019-06-06 20:00:00|           NULL|         NULL|          24.0|\n",
            "|2019-06-06 21:00:00|           NULL|         NULL|          22.0|\n",
            "|2019-06-06 22:00:00|           NULL|         NULL|          24.0|\n",
            "|2019-06-06 23:00:00|           NULL|         NULL|          21.0|\n",
            "|2019-06-07 00:00:00|           NULL|         NULL|          21.0|\n",
            "|2019-06-07 01:00:00|           NULL|         NULL|          23.0|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dff = df.na.fill(value=0, subset=[\"station_paris\"])\n",
        "dff.filter(df.station_paris==0.0).show()\n",
        "dff.show()"
      ],
      "metadata": {
        "id": "EBjXI7sffHSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9c4a95-79c6-4c46-e355-0f597da64e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------+-------------+--------------+\n",
            "|           datetime|station_antwerp|station_paris|station_london|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|2019-05-15 11:00:00|           NULL|          0.0|          36.0|\n",
            "|2019-05-15 12:00:00|           NULL|          0.0|          35.0|\n",
            "|2019-05-15 13:00:00|           NULL|          0.0|          30.0|\n",
            "|2019-05-29 16:00:00|           NULL|          0.0|           5.0|\n",
            "|2019-05-29 17:00:00|           NULL|          0.0|           3.0|\n",
            "|2019-06-12 12:00:00|           NULL|          0.0|          35.0|\n",
            "|2019-06-12 13:00:00|           NULL|          0.0|          33.0|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|           datetime|station_antwerp|station_paris|station_london|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|2019-05-07 02:00:00|           NULL|          0.0|          23.0|\n",
            "|2019-05-07 03:00:00|           50.5|         25.0|          19.0|\n",
            "|2019-05-07 04:00:00|           45.0|         27.7|          19.0|\n",
            "|2019-05-07 05:00:00|           NULL|         50.4|          16.0|\n",
            "|2019-05-07 06:00:00|           NULL|         61.9|          NULL|\n",
            "|2019-05-07 07:00:00|           NULL|         72.4|          26.0|\n",
            "|2019-05-07 08:00:00|           NULL|         77.7|          32.0|\n",
            "|2019-05-07 09:00:00|           NULL|         67.9|          32.0|\n",
            "|2019-05-07 10:00:00|           NULL|         56.0|          28.0|\n",
            "|2019-05-07 11:00:00|           NULL|         34.5|          21.0|\n",
            "|2019-05-07 12:00:00|           NULL|         20.1|          21.0|\n",
            "|2019-05-07 13:00:00|           NULL|         13.0|          18.0|\n",
            "|2019-05-07 14:00:00|           NULL|         10.6|          20.0|\n",
            "|2019-05-07 15:00:00|           NULL|         13.2|          18.0|\n",
            "|2019-05-07 16:00:00|           NULL|         11.0|          20.0|\n",
            "|2019-05-07 17:00:00|           NULL|         11.7|          20.0|\n",
            "|2019-05-07 18:00:00|           NULL|         18.2|          21.0|\n",
            "|2019-05-07 19:00:00|           NULL|         22.3|          20.0|\n",
            "|2019-05-07 20:00:00|           NULL|         21.4|          20.0|\n",
            "|2019-05-07 21:00:00|           NULL|         26.8|          24.0|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.na.fill(\"unknown\",[\"station_paris\"]) \\\n",
        "    .na.fill(\" \",[\"station_london\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQHWGJjIrZR9",
        "outputId": "c2631867-d0d9-45b5-cec5-2ad125818aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------+-------------+--------------+\n",
            "|           datetime|station_antwerp|station_paris|station_london|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|2019-05-07 02:00:00|           NULL|         NULL|          23.0|\n",
            "|2019-05-07 03:00:00|           50.5|         25.0|          19.0|\n",
            "|2019-05-07 04:00:00|           45.0|         27.7|          19.0|\n",
            "|2019-05-07 05:00:00|           NULL|         50.4|          16.0|\n",
            "|2019-05-07 06:00:00|           NULL|         61.9|          NULL|\n",
            "|2019-05-07 07:00:00|           NULL|         72.4|          26.0|\n",
            "|2019-05-07 08:00:00|           NULL|         77.7|          32.0|\n",
            "|2019-05-07 09:00:00|           NULL|         67.9|          32.0|\n",
            "|2019-05-07 10:00:00|           NULL|         56.0|          28.0|\n",
            "|2019-05-07 11:00:00|           NULL|         34.5|          21.0|\n",
            "|2019-05-07 12:00:00|           NULL|         20.1|          21.0|\n",
            "|2019-05-07 13:00:00|           NULL|         13.0|          18.0|\n",
            "|2019-05-07 14:00:00|           NULL|         10.6|          20.0|\n",
            "|2019-05-07 15:00:00|           NULL|         13.2|          18.0|\n",
            "|2019-05-07 16:00:00|           NULL|         11.0|          20.0|\n",
            "|2019-05-07 17:00:00|           NULL|         11.7|          20.0|\n",
            "|2019-05-07 18:00:00|           NULL|         18.2|          21.0|\n",
            "|2019-05-07 19:00:00|           NULL|         22.3|          20.0|\n",
            "|2019-05-07 20:00:00|           NULL|         21.4|          20.0|\n",
            "|2019-05-07 21:00:00|           NULL|         26.8|          24.0|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.na.fill({\"station_paris\": \"unknown\", \"station_london\": \"\"}) \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77hMVq9StMOn",
        "outputId": "d00a6773-c88e-4303-e080-040d5dfa2fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------+-------------+--------------+\n",
            "|           datetime|station_antwerp|station_paris|station_london|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|2019-05-07 02:00:00|           NULL|         NULL|          23.0|\n",
            "|2019-05-07 03:00:00|           50.5|         25.0|          19.0|\n",
            "|2019-05-07 04:00:00|           45.0|         27.7|          19.0|\n",
            "|2019-05-07 05:00:00|           NULL|         50.4|          16.0|\n",
            "|2019-05-07 06:00:00|           NULL|         61.9|          NULL|\n",
            "|2019-05-07 07:00:00|           NULL|         72.4|          26.0|\n",
            "|2019-05-07 08:00:00|           NULL|         77.7|          32.0|\n",
            "|2019-05-07 09:00:00|           NULL|         67.9|          32.0|\n",
            "|2019-05-07 10:00:00|           NULL|         56.0|          28.0|\n",
            "|2019-05-07 11:00:00|           NULL|         34.5|          21.0|\n",
            "|2019-05-07 12:00:00|           NULL|         20.1|          21.0|\n",
            "|2019-05-07 13:00:00|           NULL|         13.0|          18.0|\n",
            "|2019-05-07 14:00:00|           NULL|         10.6|          20.0|\n",
            "|2019-05-07 15:00:00|           NULL|         13.2|          18.0|\n",
            "|2019-05-07 16:00:00|           NULL|         11.0|          20.0|\n",
            "|2019-05-07 17:00:00|           NULL|         11.7|          20.0|\n",
            "|2019-05-07 18:00:00|           NULL|         18.2|          21.0|\n",
            "|2019-05-07 19:00:00|           NULL|         22.3|          20.0|\n",
            "|2019-05-07 20:00:00|           NULL|         21.4|          20.0|\n",
            "|2019-05-07 21:00:00|           NULL|         26.8|          24.0|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dff = df.null.fill(value=\"unknown\", subset=[\"station_antwerp\"])\n",
        "# dff.filter(df.station_paris==\"unknown\").show()\n",
        "dff.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "gvpgzxwUunN4",
        "outputId": "e78d4bb1-7bf8-4218-8d40-c6f00164a030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'null'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-aa135d4aa2d6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"station_antwerp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# dff.filter(df.station_paris==\"unknown\").show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3125\u001b[0m         \"\"\"\n\u001b[1;32m   3126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3127\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3128\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3129\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'null'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "null_count = df.filter(col('station_antwerp').isNull()).count()\n",
        "print(\"Null count in 'station_antwerp' column:\", null_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2RPonnSvcXW",
        "outputId": "5c8db5ed-2357-4ec1-8608-e234924ca9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null count in 'station_antwerp' column: 940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check for null and empty string values in 'station_antwerp' column\n",
        "null_count = df.filter((col('station_antwerp').isNull()) | (col('station_antwerp') == '')).count()\n",
        "\n",
        "# If there are null or empty string values, replace them with \"unknown\"\n",
        "if null_count > 0:\n",
        "    dff = df.na.fill(value=\"unknown\", subset=[\"station_antwerp\"])\n",
        "    dff.show()\n",
        "else:\n",
        "    print(\"No null or empty string values found in 'station_antwerp' column.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cJIug9avfcA",
        "outputId": "07a23912-4d78-4ac0-f4f6-a33adf5a8e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------+-------------+--------------+\n",
            "|           datetime|station_antwerp|station_paris|station_london|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "|2019-05-07 02:00:00|           NULL|         NULL|          23.0|\n",
            "|2019-05-07 03:00:00|           50.5|         25.0|          19.0|\n",
            "|2019-05-07 04:00:00|           45.0|         27.7|          19.0|\n",
            "|2019-05-07 05:00:00|           NULL|         50.4|          16.0|\n",
            "|2019-05-07 06:00:00|           NULL|         61.9|          NULL|\n",
            "|2019-05-07 07:00:00|           NULL|         72.4|          26.0|\n",
            "|2019-05-07 08:00:00|           NULL|         77.7|          32.0|\n",
            "|2019-05-07 09:00:00|           NULL|         67.9|          32.0|\n",
            "|2019-05-07 10:00:00|           NULL|         56.0|          28.0|\n",
            "|2019-05-07 11:00:00|           NULL|         34.5|          21.0|\n",
            "|2019-05-07 12:00:00|           NULL|         20.1|          21.0|\n",
            "|2019-05-07 13:00:00|           NULL|         13.0|          18.0|\n",
            "|2019-05-07 14:00:00|           NULL|         10.6|          20.0|\n",
            "|2019-05-07 15:00:00|           NULL|         13.2|          18.0|\n",
            "|2019-05-07 16:00:00|           NULL|         11.0|          20.0|\n",
            "|2019-05-07 17:00:00|           NULL|         11.7|          20.0|\n",
            "|2019-05-07 18:00:00|           NULL|         18.2|          21.0|\n",
            "|2019-05-07 19:00:00|           NULL|         22.3|          20.0|\n",
            "|2019-05-07 20:00:00|           NULL|         21.4|          20.0|\n",
            "|2019-05-07 21:00:00|           NULL|         26.8|          24.0|\n",
            "+-------------------+---------------+-------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "filePath=\"/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2_long.csv\"\n",
        "df = spark.read.options(header='true', inferSchema='true') \\\n",
        "          .csv(filePath)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "\n",
        "df.fillna(value=0).show()\n",
        "df.fillna(value=0,subset=[\"location\"]).show()\n",
        "df.na.fill(value=0).show()\n",
        "df.na.fill(value=0,subset=[\"value\"]).show()\n",
        "\n",
        "\n",
        "df.fillna(value=\"\").show()\n",
        "df.na.fill(value=\"\").show()\n",
        "\n",
        "df.fillna(\"unknown\",[\"location\"]) \\\n",
        "    .fillna(\"\",[\"parameter\"]).show()\n",
        "\n",
        "df.fillna({\"value\": \"unknown\", \"parameter\": \"\"}) \\\n",
        "    .show()\n",
        "\n",
        "df.na.fill(\"unknown\",[\"value\"]) \\\n",
        "    .na.fill(\"\",[\"parameter\"]).show()\n",
        "\n",
        "df.na.fill({\"value\": \"unknown\", \"parameter\": \"\"}) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdLaDxyawVmP",
        "outputId": "d2f411e7-b78c-4bc4-f914-a10f1208c76a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- city: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- date.utc: timestamp (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- parameter: string (nullable = true)\n",
            " |-- value: double (nullable = true)\n",
            " |-- unit: string (nullable = true)\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|city |country|date.utc           |location|parameter|value|unit |\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|FR     |2019-06-21 00:00:00|FR04014 |no2      |20.0 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 23:00:00|FR04014 |no2      |21.8 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 22:00:00|FR04014 |no2      |26.5 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 21:00:00|FR04014 |no2      |24.9 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 20:00:00|FR04014 |no2      |21.4 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 19:00:00|FR04014 |no2      |25.3 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 18:00:00|FR04014 |no2      |23.9 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 17:00:00|FR04014 |no2      |23.2 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 16:00:00|FR04014 |no2      |19.0 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 15:00:00|FR04014 |no2      |19.3 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 14:00:00|FR04014 |no2      |20.1 |µg/m³|\n",
            "|Paris|FR     |2019-06-20 13:00:00|FR04014 |no2      |19.4 |µg/m³|\n",
            "|Paris|FR     |2019-06-19 10:00:00|FR04014 |no2      |26.6 |µg/m³|\n",
            "|Paris|FR     |2019-06-19 09:00:00|FR04014 |no2      |27.3 |µg/m³|\n",
            "|Paris|FR     |2019-06-18 22:00:00|FR04014 |no2      |39.3 |µg/m³|\n",
            "|Paris|FR     |2019-06-18 21:00:00|FR04014 |no2      |23.1 |µg/m³|\n",
            "|Paris|FR     |2019-06-18 20:00:00|FR04014 |no2      |17.0 |µg/m³|\n",
            "|Paris|FR     |2019-06-18 19:00:00|FR04014 |no2      |15.3 |µg/m³|\n",
            "|Paris|FR     |2019-06-18 08:00:00|FR04014 |no2      |49.6 |µg/m³|\n",
            "|Paris|FR     |2019-06-18 07:00:00|FR04014 |no2      |52.6 |µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "| city|country|           date.utc|location|parameter|value| unit|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "|Paris|     FR|2019-06-21 00:00:00| FR04014|      no2| 20.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 23:00:00| FR04014|      no2| 21.8|µg/m³|\n",
            "|Paris|     FR|2019-06-20 22:00:00| FR04014|      no2| 26.5|µg/m³|\n",
            "|Paris|     FR|2019-06-20 21:00:00| FR04014|      no2| 24.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 20:00:00| FR04014|      no2| 21.4|µg/m³|\n",
            "|Paris|     FR|2019-06-20 19:00:00| FR04014|      no2| 25.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 18:00:00| FR04014|      no2| 23.9|µg/m³|\n",
            "|Paris|     FR|2019-06-20 17:00:00| FR04014|      no2| 23.2|µg/m³|\n",
            "|Paris|     FR|2019-06-20 16:00:00| FR04014|      no2| 19.0|µg/m³|\n",
            "|Paris|     FR|2019-06-20 15:00:00| FR04014|      no2| 19.3|µg/m³|\n",
            "|Paris|     FR|2019-06-20 14:00:00| FR04014|      no2| 20.1|µg/m³|\n",
            "|Paris|     FR|2019-06-20 13:00:00| FR04014|      no2| 19.4|µg/m³|\n",
            "|Paris|     FR|2019-06-19 10:00:00| FR04014|      no2| 26.6|µg/m³|\n",
            "|Paris|     FR|2019-06-19 09:00:00| FR04014|      no2| 27.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 22:00:00| FR04014|      no2| 39.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 21:00:00| FR04014|      no2| 23.1|µg/m³|\n",
            "|Paris|     FR|2019-06-18 20:00:00| FR04014|      no2| 17.0|µg/m³|\n",
            "|Paris|     FR|2019-06-18 19:00:00| FR04014|      no2| 15.3|µg/m³|\n",
            "|Paris|     FR|2019-06-18 08:00:00| FR04014|      no2| 49.6|µg/m³|\n",
            "|Paris|     FR|2019-06-18 07:00:00| FR04014|      no2| 52.6|µg/m³|\n",
            "+-----+-------+-------------------+--------+---------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwuAHpQmxR6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pivot and Unpivot**"
      ],
      "metadata": {
        "id": "xSOS83u-xT1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark pivot() function is used to rotate/transpose the data from one column into multiple Dataframe columns and back using unpivot(). Pivot() It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data.**"
      ],
      "metadata": {
        "id": "OOTbfnmjxQLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "#Create spark session\n",
        "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
        "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
        "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
        "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
        "\n",
        "columns= [\"Product\",\"Amount\",\"Country\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5btWlEZVwV_X",
        "outputId": "bfd23d5e-7c45-495b-bd1f-5c3dcdea3284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Amount: long (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "+-------+------+-------+\n",
            "|Product|Amount|Country|\n",
            "+-------+------+-------+\n",
            "| Banana|  1000|    USA|\n",
            "|Carrots|  1500|    USA|\n",
            "|  Beans|  1600|    USA|\n",
            "| Orange|  2000|    USA|\n",
            "| Orange|  2000|    USA|\n",
            "| Banana|   400|  China|\n",
            "|Carrots|  1200|  China|\n",
            "|  Beans|  1500|  China|\n",
            "| Orange|  4000|  China|\n",
            "| Banana|  2000| Canada|\n",
            "|Carrots|  2000| Canada|\n",
            "|  Beans|  2000| Mexico|\n",
            "+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
        "pivotDF.printSchema()\n",
        "pivotDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLG0jAW-yds-",
        "outputId": "f317a8d8-15b5-4379-b095-986f793f9004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Canada: long (nullable = true)\n",
            " |-- China: long (nullable = true)\n",
            " |-- Mexico: long (nullable = true)\n",
            " |-- USA: long (nullable = true)\n",
            "\n",
            "+-------+------+-----+------+----+\n",
            "|Product|Canada|China|Mexico|USA |\n",
            "+-------+------+-----+------+----+\n",
            "|Orange |NULL  |4000 |NULL  |4000|\n",
            "|Beans  |NULL  |1500 |2000  |1600|\n",
            "|Banana |2000  |400  |NULL  |1000|\n",
            "|Carrots|2000  |1200 |NULL  |1500|\n",
            "+-------+------+-----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "countries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\n",
        "pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
        "pivotDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu8wtwJZyqIz",
        "outputId": "cfc161ef-f58c-4ee4-b8b4-1bca6d903770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+------+------+\n",
            "|Product|USA |China|Canada|Mexico|\n",
            "+-------+----+-----+------+------+\n",
            "|Orange |4000|4000 |NULL  |NULL  |\n",
            "|Beans  |1600|1500 |NULL  |2000  |\n",
            "|Banana |1000|400  |2000  |NULL  |\n",
            "|Carrots|1500|1200 |2000  |NULL  |\n",
            "+-------+----+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\" unpivot \"\"\"\n",
        "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
        "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
        "    .where(\"Total is not null\")\n",
        "unPivotDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDXbB2p2y8Y1",
        "outputId": "6b24f4d6-b38c-43f8-c247-5147c4718f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|Product|Country|Total|\n",
            "+-------+-------+-----+\n",
            "|Orange |China  |4000 |\n",
            "|Beans  |China  |1500 |\n",
            "|Beans  |Mexico |2000 |\n",
            "|Banana |Canada |2000 |\n",
            "|Banana |China  |400  |\n",
            "|Carrots|Canada |2000 |\n",
            "|Carrots|China  |1200 |\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PartitionBy()**"
      ],
      "metadata": {
        "id": "9PzGqrcg0-xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark partitionBy() is a function of pyspark.sql.DataFrameWriter class which is used to partition the large dataset (DataFrame) into smaller files based on one or multiple columns while writing to disk,"
      ],
      "metadata": {
        "id": "KfVd_rLG1ERz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9pS9uiozrpo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}