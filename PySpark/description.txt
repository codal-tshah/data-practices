11_apr:
    pyspark Intro:
    - PySpark is a Python API to support Python with Apache Spark
    - PySpark provides Py4j library
    - Spark is an open-source, cluster computing system which is used for big data solution

12_apr: 
- PySpark 
  - Configured PySpark environment.
  - Utilized SparkConf for application configuration.
  - Encountered connectivity issues with Google Colab and local runtime.
  - Configured Jupyter Notebook for PySpark.
  - Commenced learning PySpark fundamentals and covered introductory concepts.

15_apr:
- PySpark:
  - SparkContext :is the entry point for interacting with Spark functionality.
    It represents the connection to a Spark cluster, and it's responsible for coordinating the execution of Spark jobs.
  - SparkSession is the entry point for working with structured data in Spark
    It provides a unified interface for interacting with Spark, including SQL, DataFrames, Datasets, and other Spark APIs.
  - reading of csv and json files 
  - Read CSV file in to Dataframe using PySpark
  - Write DataFrame into CSV file using PySpark
  - Read json file into DataFrame using Pyspark 
  - Write DataFrame into json file using PySpark 
  - Read multiple json files from folder
  - Read Parquet file into Dataframe using PySpark 
  - Write DataFrame into parquet file using PySpark 
  - show() in Pyspark to display Dataframe contents 
  - withColumn() in PySpark I Add new column or Change existing column data or type in DataFrame
  - withColumnRenamed() usage in PySpark

16_apr:
- Pyspark:
  - StructType – Defines the structure of the DataFrame
  - StructField – Defines the metadata of the DataFrame column
  - PySpark ArrayType():
    - Use explode() function to create a new row for each element in the given array column
    - split() sql function returns an array type after splitting the string column by delimiter. 
    - Use array() function to create a new array column by merging the data from multiple columns. All input columns must have the same data type.
    - array_contains() sql function is used to check if array column contains a value. Returns null if the array is null, true if the array contains the value, and false otherwise.

17_apr:
- local – which is not really a cluster manager but still I wanted to mention that we use “local” for master() in order to run Spark on your laptop/computer.
- PySpark When Otherwise – when() is a SQL function that returns a Column type and otherwise() is a function of Column, if otherwise() is not used, it returns a None/NULL value.
- PySpark SQL Case When – This is similar to SQL expression, Usage: CASE WHEN cond1 THEN result WHEN cond2 THEN result... ELSE result END.
- Learned about Hadoop and map Reduce concepts relates to PySpark
- Learned about Spark engine workflows in which it has master and worker node like concepts.

18_apr:
- Create an empty DataFrame
- Convert RDD to DataFrame
- Convert DataFrame to Pandas
- show()
- StructType & StructFie1d
- Column Class

22_apr:
- PySpark Collect() – Retrieve data from DataFrame
  - 