# -*- coding: utf-8 -*-
"""PySpark Datasources.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hxv-3V7Uf6bmB3S5mHZ8tPjJDWyKxsgX
"""

# !pip install pyspark

from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("new_app.com").getOrCreate()

"""# **Read & Write CSV Files**"""

df = spark.read.csv("/content/drive/MyDrive/data_practices/pandas/data/titanic.csv")
df.printSchema()

df.show()

df = spark.read.format("csv").load(
    "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv"
)
df.printSchema()
df.show()

"""lets make correct header"""

df2 = spark.read.option("header", True).csv(
    "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv"
)
df2.printSchema()
df2.show()

"""**Read Multiple CSV Files**"""

mul_df = spark.read.option("header", True).csv(
    [
        "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv",
        "/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2_long.csv",
    ]
)
mul_df.printSchema()
mul_df.show()

df = spark.read.csv("/content/drive/MyDrive/data_practices/pandas/data")
df.printSchema()
df.show()

df3 = spark.read.options(delimiter=",").csv(
    "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv"
)
df3.printSchema()
df3.show(truncate=False)

from pyspark.sql.functions import split

# Read the CSV file with the default delimiter (",")
df3 = spark.read.csv(
    "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv", header=True
)

# Split the columns based on the custom delimiter ("++")
df3 = df3.select([split(col, "|") for col in df3.columns])

# Show the DataFrame
df3.show(truncate=False)

"""**The default value set to this option is False when setting to true it automatically infers column types based on the data.**"""

df4 = spark.read.options(inferSchema="True", delimiter=",").csv(
    "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv"
)

"""**Reading CSV files with a user-specified custom schema**"""

schema = (
    StructType()
    .add("RecordNumber", IntegerType(), True)
    .add("Zipcode", IntegerType(), True)
    .add("ZipCodeType", StringType(), True)
    .add("City", StringType(), True)
    .add("State", StringType(), True)
    .add("LocationType", StringType(), True)
    .add("Lat", DoubleType(), True)
    .add("Long", DoubleType(), True)
    .add("Xaxis", IntegerType(), True)
    .add("Yaxis", DoubleType(), True)
    .add("Zaxis", DoubleType(), True)
    .add("WorldRegion", StringType(), True)
    .add("Country", StringType(), True)
    .add("LocationText", StringType(), True)
    .add("Location", StringType(), True)
    .add("Decommisioned", BooleanType(), True)
    .add("TaxReturnsFiled", StringType(), True)
    .add("EstimatedPopulation", IntegerType(), True)
    .add("TotalWages", IntegerType(), True)
    .add("Notes", StringType(), True)
)

df_with_schema = (
    spark.read.format("csv")
    .option("header", True)
    .schema(schema)
    .load("/content/drive/MyDrive/data_practices/pandas/data/air_quality_pm25_long.csv")
)

df_with_schema.show()
df_with_schema.printSchema()

df_with_schema.write.options(header="True", delimiter=",").csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_pm25_long"
)

df_with_schema.write.mode("overwrite").csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_pm25_long"
)

df_with_schema.write.option("header", True).csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_pm25_long"
)


"""# **Read and Write Parquet File**

Apache Parquet file is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or programming language.
"""

data = [
    ("James ", "", "Smith", "36636", "M", 3000),
    ("Michael ", "Rose", "", "40288", "M", 4000),
    ("Robert ", "", "Williams", "42114", "M", 4000),
    ("Maria ", "Anne", "Jones", "39192", "F", 4000),
    ("Jen", "Mary", "Brown", "", "F", -1),
]
columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data, columns)

df.write.parquet("/content/drive/MyDrive/data_practices/pyspark/data/parquet_files_1")

pardf = spark.read.parquet(
    "/content/drive/MyDrive/data_practices/pyspark/data/parquet_files_1"
)
pardf.show()

df.write.mode("append").parquet(
    "/content/drive/MyDrive/data_practices/pyspark/data/parquet_files_1"
)
df.write.mode("overwrite").parquet(
    "/content/drive/MyDrive/data_practices/pyspark/data/parquet_files_1"
)

pardf.createOrReplaceTempView("ParquetTable")

parkSQL = spark.sql("select * from ParquetTable where salary >= 4000 ")

df.write.partitionBy("gender", "salary").mode("overwrite").parquet(
    "/content/drive/MyDrive/data_practices/pyspark/data/parquet_files_1.parquet"
)

parDF2 = spark.read.parquet(
    "/content/drive/MyDrive/data_practices/pyspark/data/parquet_files_1.parquet/gender=M"
)
parDF2.show(truncate=False)

# Create a temporary view on partitioned Parquet file
spark.sql(
    'CREATE TEMPORARY VIEW PERSON2 USING parquet OPTIONS (path "/content/drive/MyDrive/data_practices/pyspark/data/parquet_files_1.parquet/gender=F")'
)
spark.sql("SELECT * FROM PERSON2").show()
