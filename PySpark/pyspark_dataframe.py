# -*- coding: utf-8 -*-
"""PySpark DataFrame.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlaE1lpnh3TCh0Tr4_0Os-ttIL5OUizs
"""

# !pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkExample.com").getOrCreate()

"""# **Create an empty DataFrame**"""

emptyRDD = spark.sparkContext.emptyRDD()
print(emptyRDD)

"""**Create Empty DataFrame with Schema (StructType)**"""

from pyspark.sql.types import StructType, StructField, StringType

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
    ]
)

df = spark.createDataFrame(emptyRDD, schema)
df.printSchema()
df.show()

"""**Convert Empty RDD to DataFrame**"""

df2 = spark.createDataFrame([], schema)
df2.printSchema()
df2.show()

"""# **Convert PySpark RDD to DataFrame**

**Create PySpark RDD**
"""

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

"""**Completed Example**"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

from pyspark.sql.types import StructType, StructField, StringType

deptSchema = StructType(
    [
        StructField("dept_name", StringType(), True),
        StructField("dept_id", StringType(), True),
    ]
)

deptDF1 = spark.createDataFrame(rdd, schema=deptSchema)
deptDF1.printSchema()
deptDF1.show(truncate=False)

"""SHOW()"""

df2.show()

df2.show(2, truncate=False)

df2.show(2, truncate=3)

df2.show(2, truncate=False, vertical=True)

print(df2.schema.json())


"""# **StructType & StructField**"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    ArrayType,
    MapType,
)
from pyspark.sql.functions import col, struct, when

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

data = [
    ("James", "", "Smith", "36636", "M", 3000),
    ("Michael", "Rose", "", "40288", "M", 4000),
    ("Robert", "", "Williams", "42114", "M", 4000),
    ("Maria", "Anne", "Jones", "39192", "F", 4000),
    ("Jen", "Mary", "Brown", "", "F", -1),
]

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df = spark.createDataFrame(data=data, schema=schema)
df.printSchema()
df.show(truncate=False)

structureData = [
    (("James", "", "Smith"), "36636", "M", 3100),
    (("Michael", "Rose", ""), "40288", "M", 4300),
    (("Robert", "", "Williams"), "42114", "M", 1400),
    (("Maria", "Anne", "Jones"), "39192", "F", 5500),
    (("Jen", "Mary", "Brown"), "", "F", -1),
]
structureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df2 = spark.createDataFrame(data=structureData, schema=structureSchema)
df2.printSchema()
df2.show(truncate=False)


updatedDF = df2.withColumn(
    "OtherInfo",
    struct(
        col("id").alias("identifier"),
        col("gender").alias("gender"),
        col("salary").alias("salary"),
        when(col("salary").cast(IntegerType()) < 2000, "Low")
        .when(col("salary").cast(IntegerType()) < 4000, "Medium")
        .otherwise("High")
        .alias("Salary_Grade"),
    ),
).drop("id", "gender", "salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)


""" Array & Map"""


arrayStructureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("hobbies", ArrayType(StringType()), True),
        StructField("properties", MapType(StringType(), StringType()), True),
    ]
)

"""# **Column Class | Operators & Functions**

**Create Column Class Object**
"""

from pyspark.sql.functions import lit

colObj = lit("sparkbyexamples.com")

data = [("James", 23), ("Ann", 40)]
df = spark.createDataFrame(data).toDF("name", "gender")
df.printSchema()

# Using DataFrame object (df)
df.select(df.gender).show()
df.select(df["gender"]).show()
# Accessing column name with dot (with backticks)
df.select(df["`name`"]).show()

# Using SQL col() function
from pyspark.sql.functions import col

df.select(col("gender")).show()
# Accessing column name with dot (with backticks)
df.select(col("`name`")).show()

data = [
    ("James", "Bond", "100", None),
    ("Ann", "Varsa", "200", "F"),
    ("Tom Cruise", "XXX", "400", ""),
    ("Tom Brand", None, "400", "M"),
]
columns = ["fname", "lname", "id", "gender"]
df = spark.createDataFrame(data, columns)
df.show()

# alias
from pyspark.sql.functions import expr

df.select(df.fname.alias("first_name"), df.lname.alias("last_name")).show()

# Another example
df.select(expr(" fname ||','|| lname").alias("fullName")).show()

df.sort(df.fname.asc()).show()
df.sort(df.fname.desc()).show()

df.filter(df.id.between(100, 300)).show()

df.filter(df.fname.contains("B")).show()

df.select(df.fname, df.lname, df.id).filter(df.fname.like("%om"))

from pyspark.sql.functions import when

df.select(
    df.fname,
    df.lname,
    when(df.gender == "M", "Male")
    .when(df.gender == "F", "Female")
    .when(df.gender == None, "")
    .otherwise(df.gender)
    .alias("new_gender"),
).show()

df.filter(df.gender.isNull()).show()

df.printSchema()

# getItem() used with ArrayType
# df.select(df.languages.getItem(1)).show()

# getItem() used with MapType
df.select(df.lname.getItem("xxx")).show()


"""# **Select Columns**"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Spark.examples.com").getOrCreate()
data = [
    ("mahesh ihfweoifow", "sci", 15, "ssoa"),
    ("ramesh", "comm", 17, "xaviers"),
    ("fenil a;h;hwo;eihfwoeih", "arts", 16, "dhl"),
    ("kenil", "sci", 18, "infocity"),
]
columns = ["Name", "stream", "Age", "School"]
df = spark.createDataFrame(data=data, schema=columns)
df.show()
df.printSchema()

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()
data = [
    ("James", "Smith", "USA", "CA"),
    ("Michael", "Rose", "USA", "NY"),
    ("Robert", "Williams", "USA", "CA"),
    ("Maria", "Jones", "USA", "FL"),
]
columns = ["firstname", "lastname", "country", "state"]
df = spark.createDataFrame(data=data, schema=columns)
df.show(truncate=False)

df.select("firstname", "lastname").show()
df.select(df.firstname, df.lastname).show()
df.select(df["firstname"], df["lastname"]).show()

# By using col() function
from pyspark.sql.functions import col

df.select(col("firstname"), col("lastname")).show()

# Select columns by regular expression
df.select(df.colRegex("`^.*name*`")).show()

df.select(*columns).show()

# Select All columns
df.select([col for col in df.columns]).show()
df.select("*").show()

# Selects first 3 columns and top 3 rows
df.select(df.columns[:3]).show(3)

# Selects columns 2 to 4  and top 3 r
df.select(df.columns[2:4]).show(3)


"""# **COLLECT()**

**collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver.**
"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
deptColumns = ["dept_name", "dept_id"]
deptDF = spark.createDataFrame(data=dept, schema=deptColumns)
deptDF.show(truncate=False)

dataCollect = deptDF.collect()
print(dataCollect)

for row in dataCollect:
    print(row["dept_name"] + "," + str(row["dept_id"]))

"""# **Where Filter**"""

from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import StringType, IntegerType, ArrayType

data = [
    (("James", "", "Smith"), ["Java", "Scala", "C++"], "OH", "M"),
    (("Anna", "Rose", ""), ["Spark", "Java", "C++"], "NY", "F"),
    (("Julia", "", "Williams"), ["CSharp", "VB"], "OH", "F"),
    (("Maria", "Anne", "Jones"), ["CSharp", "VB"], "NY", "M"),
    (("Jen", "Mary", "Brown"), ["CSharp", "VB"], "NY", "M"),
    (("Mike", "Mary", "Williams"), ["Python", "VB"], "OH", "M"),
]

schema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("languages", ArrayType(StringType()), True),
        StructField("state", StringType(), True),
        StructField("gender", StringType(), True),
    ]
)

df = spark.createDataFrame(data=data, schema=schema)
df.printSchema()
df.show(truncate=False)

df.filter(df.state == "OH").show(truncate=False)

# not equals condition
df.filter(df.state != "OH").show(truncate=False)
df.filter(~(df.state == "OH")).show(truncate=False)

df.filter("gender <> 'M'").show()

li = ["OH", "CA", "DE"]
df.filter(df.state.isin(li)).show()

# Filter NOT IS IN List values
# These show all records with NY (NY is not part of the list)
df.filter(~df.state.isin(li)).show()
df.filter(df.state.isin(li) == False).show()

# Prepare Data
data2 = [
    (2, "Michael Rose"),
    (3, "Robert Williams"),
    (4, "Rames Rose"),
    (5, "Rames rose"),
]
df2 = spark.createDataFrame(data=data2, schema=["id", "name"])

# like - SQL LIKE pattern
df2.filter(df2.name.like("%rose%")).show()

df2.filter(df2.name.rlike("(?i)^*rose$")).show()

"""# **Distinct to Drop Duplicate**

distinct() transformation is used to drop/remove the duplicate rows (all columns) from DataFrame and dropDuplicates() is used to drop rows based on selected (one or multiple) columns.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

data = [
    ("James", "Sales", 3000),
    ("Michael", "Sales", 4600),
    ("Robert", "Sales", 4100),
    ("Maria", "Finance", 3000),
    ("James", "Sales", 3000),
    ("Scott", "Finance", 3300),
    ("Jen", "Finance", 3900),
    ("Jeff", "Marketing", 3000),
    ("Kumar", "Marketing", 2000),
    ("Saif", "Sales", 4100),
]
columns = ["employee_name", "department", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()
df.show(truncate=False)

# Distinct
distinctDF = df.distinct()
print("Distinct count: " + str(distinctDF.count()))
distinctDF.show(truncate=False)

# Drop duplicates
df2 = df.dropDuplicates()
print("Distinct count: " + str(df2.count()))
df2.show(truncate=False)

# Drop duplicates on selected columns
dropDisDF = df.dropDuplicates(["department", "salary"])
print("Distinct count of department salary : " + str(dropDisDF.count()))
dropDisDF.show(truncate=False)

"""# **MAP()**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

data = [
    "Project",
    "Gutenberg’s",
    "Alice’s",
    "Adventures",
    "in",
    "Wonderland",
    "Project",
    "Gutenberg’s",
    "Adventures",
    "in",
    "Wonderland",
    "Project",
    "Gutenberg’s",
]

rdd = spark.sparkContext.parallelize(data)

rdd2 = rdd.map(lambda x: (x, 1))
for element in rdd2.collect():
    print(element)

data = [
    ("James", "Smith", "M", 30),
    ("Anna", "Rose", "F", 41),
    ("Robert", "Williams", "M", 62),
]

columns = ["firstname", "lastname", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.show()

rdd2 = df.rdd.map(lambda x: (x[0] + "," + x[1], x[2], x[3] * 2))
df2 = rdd2.toDF(["name", "gender", "new_salary"])
df2.show()

# Referring Column Names
rdd2 = df.rdd.map(
    lambda x: (x["firstname"] + "," + x["lastname"], x["gender"], x["salary"] * 2)
)

# Referring Column Names
rdd2 = df.rdd.map(lambda x: (x.firstname + "," + x.lastname, x.gender, x.salary * 2))


def func1(x):
    firstName = x.firstname
    lastName = x.lastname
    name = firstName + "," + lastName
    gender = x.gender.lower()
    salary = x.salary * 2
    return (name, gender, salary)


rdd2 = df.rdd.map(lambda x: func1(x))

"""# **FLAT MAP()**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

data = [
    "Project Gutenberg’s",
    "Alice’s Adventures in Wonderland",
    "Project Gutenberg’s",
    "Adventures in Wonderland",
    "Project Gutenberg’s",
]
rdd = spark.sparkContext.parallelize(data)
for element in rdd.collect():
    print(element)

# Flatmap
rdd2 = rdd.flatMap(lambda x: x.split(" "))
for element in rdd2.collect():
    print(element)

"""**Unfortunately, PySpark DataFame doesn’t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column.**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("pyspark-by-examples").getOrCreate()

arrayData = [
    ("James", ["Java", "Scala"], {"hair": "black", "eye": "brown"}),
    ("Michael", ["Spark", "Java", None], {"hair": "brown", "eye": None}),
    ("Robert", ["CSharp", ""], {"hair": "red", "eye": ""}),
    ("Washington", None, None),
    ("Jefferson", ["1", "2"], {}),
]
df = spark.createDataFrame(
    data=arrayData, schema=["name", "knownLanguages", "properties"]
)

from pyspark.sql.functions import explode

df2 = df.select(df.name, explode(df.knownLanguages))
df2.printSchema()
df2.show()

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

df = spark.range(100)
print(df.sample(0.05, 123).collect())


"""# **orderBy() and sort()**"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, asc, desc

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

simpleData = [
    ("James", "Sales", "NY", 90000, 34, 10000),
    ("Michael", "Sales", "NY", 86000, 56, 20000),
    ("Robert", "Sales", "CA", 81000, 30, 23000),
    ("Maria", "Finance", "CA", 90000, 24, 23000),
    ("Raman", "Finance", "CA", 99000, 40, 24000),
    ("Scott", "Finance", "NY", 83000, 36, 19000),
    ("Jen", "Finance", "NY", 79000, 53, 15000),
    ("Jeff", "Marketing", "CA", 80000, 25, 18000),
    ("Kumar", "Marketing", "NY", 91000, 50, 21000),
]
columns = ["employee_name", "department", "state", "salary", "age", "bonus"]

df = spark.createDataFrame(data=simpleData, schema=columns)

df.printSchema()
df.show(truncate=False)

df.sort("department", "state").show(truncate=False)
df.sort(col("department"), col("state")).show(truncate=False)

df.orderBy("department", "state").show(truncate=False)
df.orderBy(col("department"), col("state")).show(truncate=False)

df.sort(df.department.asc(), df.state.asc()).show(truncate=False)
df.sort(col("department").asc(), col("state").asc()).show(truncate=False)
df.orderBy(col("department").asc(), col("state").asc()).show(truncate=False)

df.sort(df.department.asc(), df.state.desc()).show(truncate=False)
df.sort(col("department").asc(), col("state").desc()).show(truncate=False)
df.orderBy(col("department").asc(), col("state").desc()).show(truncate=False)

df.createOrReplaceTempView("EMP")
spark.sql(
    "select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc"
).show(truncate=False)

"""# **Union and UnionAll**"""

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

simpleData = [
    ("James", "Sales", "NY", 90000, 34, 10000),
    ("Michael", "Sales", "NY", 86000, 56, 20000),
    ("Robert", "Sales", "CA", 81000, 30, 23000),
    ("Maria", "Finance", "CA", 90000, 24, 23000),
]

columns = ["employee_name", "department", "state", "salary", "age", "bonus"]
df = spark.createDataFrame(data=simpleData, schema=columns)
df.printSchema()
df.show(truncate=False)

simpleData2 = [
    ("James", "Sales", "NY", 90000, 34, 10000),
    ("Maria", "Finance", "CA", 90000, 24, 23000),
    ("Jen", "Finance", "NY", 79000, 53, 15000),
    ("Jeff", "Marketing", "CA", 80000, 25, 18000),
    ("Kumar", "Marketing", "NY", 91000, 50, 21000),
]
columns2 = ["employee_name", "department", "state", "salary", "age", "bonus"]

df2 = spark.createDataFrame(data=simpleData2, schema=columns2)

df2.printSchema()
df2.show(truncate=False)

unionDF = df.union(df2)
unionDF.show(truncate=False)
disDF = df.union(df2).distinct()
disDF.show(truncate=False)

unionAllDF = df.unionAll(df2)
unionAllDF.show(truncate=False)

"""# **Transformation**

- pyspark.sql.DataFrame.transform() – Available since Spark 3.0
- pyspark.sql.functions.transform()
"""

# Imports
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

# Prepare Data
simpleData = (
    ("Java", 4000, 5),
    ("Python", 4600, 10),
    ("Scala", 4100, 15),
    ("Scala", 4500, 15),
    ("PHP", 3000, 20),
)
columns = ["CourseName", "fee", "discount"]

# Create DataFrame
df = spark.createDataFrame(data=simpleData, schema=columns)
df.printSchema()
df.show(truncate=False)

from pyspark.sql.functions import upper


def to_upper_str_columns(df):
    return df.withColumn("CourseName", upper(df.CourseName))


# Custom transformation 2
def reduce_price(df, reduceBy):
    return df.withColumn("new_fee", df.fee - reduceBy)


# Custom transformation 3
def apply_discount(df):
    return df.withColumn(
        "discounted_fee", df.new_fee - (df.new_fee * df.discount) / 100
    )


df2 = (
    df.transform(to_upper_str_columns)
    .transform(reduce_price, 1000)
    .transform(apply_discount)
)

df2.show()

from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

# Prepare Data
simpleData = (
    ("Java", 4000, 5),
    ("Python", 4600, 10),
    ("Scala", 4100, 15),
    ("Scala", 4500, 15),
    ("PHP", 3000, 20),
)
columns = ["CourseName", "fee", "discount"]

# Create DataFrame
df = spark.createDataFrame(data=simpleData, schema=columns)
df.printSchema()
df.show(truncate=False)

# Custom transformation 1
from pyspark.sql.functions import upper


def to_upper_str_columns(df):
    return df.withColumn("CourseName", upper(df.CourseName))


# Custom transformation 2
def reduce_price(df, reduceBy):
    return df.withColumn("new_fee", df.fee - reduceBy)


# Custom transformation 3
def apply_discount(df):
    return df.withColumn(
        "discounted_fee", df.new_fee - (df.new_fee * df.discount) / 100
    )


# transform() usage
df2 = (
    df.transform(to_upper_str_columns)
    .transform(reduce_price, 1000)
    .transform(apply_discount)
)

df2.show()

# Create DataFrame with Array
data = [
    ("James,,Smith", ["Java", "Scala", "C++"], ["Spark", "Java"]),
    ("Michael,Rose,", ["Spark", "Java", "C++"], ["Spark", "Java"]),
    ("Robert,,Williams", ["CSharp", "VB"], ["Spark", "Python"]),
]
df = spark.createDataFrame(data=data, schema=["Name", "Languages1", "Languages2"])
df.printSchema()
df.show()

# using transform() SQL function
from pyspark.sql.functions import upper
from pyspark.sql.functions import transform

df.select(transform("Languages1", lambda x: upper(x)).alias("languages1")).show()

"""# **fillna() & fill()**"""

# Create SparkSession and read csv
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

filePath = "/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2.csv"
df = spark.read.options(header="true", inferSchema="true").csv(filePath)

df.printSchema()
df.show(truncate=False)

from pyspark.sql.functions import col, isnan, when, count

df.filter(df.station_paris.isNull()).show()

dff = df.na.fill(value=0, subset=["station_paris"])
dff.filter(df.station_paris == 0.0).show()
dff.show()

df.na.fill("unknown", ["station_paris"]).na.fill(" ", ["station_london"]).show()

df.na.fill({"station_paris": "unknown", "station_london": ""}).show()

dff = df.null.fill(value="unknown", subset=["station_antwerp"])
# dff.filter(df.station_paris=="unknown").show()
dff.show()

null_count = df.filter(col("station_antwerp").isNull()).count()
print("Null count in 'station_antwerp' column:", null_count)

# Check for null and empty string values in 'station_antwerp' column
null_count = df.filter(
    (col("station_antwerp").isNull()) | (col("station_antwerp") == "")
).count()

# If there are null or empty string values, replace them with "unknown"
if null_count > 0:
    dff = df.na.fill(value="unknown", subset=["station_antwerp"])
    dff.show()
else:
    print("No null or empty string values found in 'station_antwerp' column.")

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

filePath = "/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2_long.csv"
df = spark.read.options(header="true", inferSchema="true").csv(filePath)

df.printSchema()
df.show(truncate=False)


df.fillna(value=0).show()
df.fillna(value=0, subset=["location"]).show()
df.na.fill(value=0).show()
df.na.fill(value=0, subset=["value"]).show()


df.fillna(value="").show()
df.na.fill(value="").show()

df.fillna("unknown", ["location"]).fillna("", ["parameter"]).show()

df.fillna({"value": "unknown", "parameter": ""}).show()

df.na.fill("unknown", ["value"]).na.fill("", ["parameter"]).show()

df.na.fill({"value": "unknown", "parameter": ""}).show()


"""# **Pivot and Unpivot**

**PySpark pivot() function is used to rotate/transpose the data from one column into multiple Dataframe columns and back using unpivot(). Pivot() It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data.**
"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

# Create spark session
data = [
    ("Banana", 1000, "USA"),
    ("Carrots", 1500, "USA"),
    ("Beans", 1600, "USA"),
    ("Orange", 2000, "USA"),
    ("Orange", 2000, "USA"),
    ("Banana", 400, "China"),
    ("Carrots", 1200, "China"),
    ("Beans", 1500, "China"),
    ("Orange", 4000, "China"),
    ("Banana", 2000, "Canada"),
    ("Carrots", 2000, "Canada"),
    ("Beans", 2000, "Mexico"),
]

columns = ["Product", "Amount", "Country"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()
df.show()

pivotDF = df.groupBy("Product").pivot("Country").sum("Amount")
pivotDF.printSchema()
pivotDF.show(truncate=False)

countries = ["USA", "China", "Canada", "Mexico"]
pivotDF = df.groupBy("Product").pivot("Country", countries).sum("Amount")
pivotDF.show(truncate=False)

""" unpivot """
unpivotExpr = (
    "stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)"
)
unPivotDF = pivotDF.select("Product", expr(unpivotExpr)).where("Total is not null")
unPivotDF.show(truncate=False)

"""# **PartitionBy()**

PySpark partitionBy() is a function of pyspark.sql.DataFrameWriter class which is used to partition the large dataset (DataFrame) into smaller files based on one or multiple columns while writing to disk,
"""
