# -*- coding: utf-8 -*-
"""PySpark DataFrame.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlaE1lpnh3TCh0Tr4_0Os-ttIL5OUizs
"""

# !pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkExample.com").getOrCreate()

"""# **Create an empty DataFrame**"""

emptyRDD = spark.sparkContext.emptyRDD()
print(emptyRDD)

"""**Create Empty DataFrame with Schema (StructType)**"""

from pyspark.sql.types import StructType, StructField, StringType

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
    ]
)

df = spark.createDataFrame(emptyRDD, schema)
df.printSchema()
df.show()

"""**Convert Empty RDD to DataFrame**"""

df2 = spark.createDataFrame([], schema)
df2.printSchema()
df2.show()

"""# **Convert PySpark RDD to DataFrame**

**Create PySpark RDD**
"""

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

"""**Completed Example**"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

from pyspark.sql.types import StructType, StructField, StringType

deptSchema = StructType(
    [
        StructField("dept_name", StringType(), True),
        StructField("dept_id", StringType(), True),
    ]
)

deptDF1 = spark.createDataFrame(rdd, schema=deptSchema)
deptDF1.printSchema()
deptDF1.show(truncate=False)

"""SHOW()"""

df2.show()

df2.show(2, truncate=False)

df2.show(2, truncate=3)

df2.show(2, truncate=False, vertical=True)

print(df2.schema.json())


"""# **StructType & StructField**"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    ArrayType,
    MapType,
)
from pyspark.sql.functions import col, struct, when

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

data = [
    ("James", "", "Smith", "36636", "M", 3000),
    ("Michael", "Rose", "", "40288", "M", 4000),
    ("Robert", "", "Williams", "42114", "M", 4000),
    ("Maria", "Anne", "Jones", "39192", "F", 4000),
    ("Jen", "Mary", "Brown", "", "F", -1),
]

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df = spark.createDataFrame(data=data, schema=schema)
df.printSchema()
df.show(truncate=False)

structureData = [
    (("James", "", "Smith"), "36636", "M", 3100),
    (("Michael", "Rose", ""), "40288", "M", 4300),
    (("Robert", "", "Williams"), "42114", "M", 1400),
    (("Maria", "Anne", "Jones"), "39192", "F", 5500),
    (("Jen", "Mary", "Brown"), "", "F", -1),
]
structureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df2 = spark.createDataFrame(data=structureData, schema=structureSchema)
df2.printSchema()
df2.show(truncate=False)


updatedDF = df2.withColumn(
    "OtherInfo",
    struct(
        col("id").alias("identifier"),
        col("gender").alias("gender"),
        col("salary").alias("salary"),
        when(col("salary").cast(IntegerType()) < 2000, "Low")
        .when(col("salary").cast(IntegerType()) < 4000, "Medium")
        .otherwise("High")
        .alias("Salary_Grade"),
    ),
).drop("id", "gender", "salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)


""" Array & Map"""


arrayStructureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("hobbies", ArrayType(StringType()), True),
        StructField("properties", MapType(StringType(), StringType()), True),
    ]
)
