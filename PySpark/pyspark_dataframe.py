# -*- coding: utf-8 -*-
"""PySpark DataFrame.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlaE1lpnh3TCh0Tr4_0Os-ttIL5OUizs
"""

# !pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkExample.com").getOrCreate()

"""# **Create an empty DataFrame**"""

emptyRDD = spark.sparkContext.emptyRDD()
print(emptyRDD)

"""**Create Empty DataFrame with Schema (StructType)**"""

from pyspark.sql.types import StructType, StructField, StringType

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
    ]
)

df = spark.createDataFrame(emptyRDD, schema)
df.printSchema()
df.show()

"""**Convert Empty RDD to DataFrame**"""

df2 = spark.createDataFrame([], schema)
df2.printSchema()
df2.show()

"""# **Convert PySpark RDD to DataFrame**

**Create PySpark RDD**
"""

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

"""**Completed Example**"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

from pyspark.sql.types import StructType, StructField, StringType

deptSchema = StructType(
    [
        StructField("dept_name", StringType(), True),
        StructField("dept_id", StringType(), True),
    ]
)

deptDF1 = spark.createDataFrame(rdd, schema=deptSchema)
deptDF1.printSchema()
deptDF1.show(truncate=False)

"""SHOW()"""

df2.show()

df2.show(2, truncate=False)

df2.show(2, truncate=3)

df2.show(2, truncate=False, vertical=True)

print(df2.schema.json())


"""# **StructType & StructField**"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    ArrayType,
    MapType,
)
from pyspark.sql.functions import col, struct, when

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

data = [
    ("James", "", "Smith", "36636", "M", 3000),
    ("Michael", "Rose", "", "40288", "M", 4000),
    ("Robert", "", "Williams", "42114", "M", 4000),
    ("Maria", "Anne", "Jones", "39192", "F", 4000),
    ("Jen", "Mary", "Brown", "", "F", -1),
]

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df = spark.createDataFrame(data=data, schema=schema)
df.printSchema()
df.show(truncate=False)

structureData = [
    (("James", "", "Smith"), "36636", "M", 3100),
    (("Michael", "Rose", ""), "40288", "M", 4300),
    (("Robert", "", "Williams"), "42114", "M", 1400),
    (("Maria", "Anne", "Jones"), "39192", "F", 5500),
    (("Jen", "Mary", "Brown"), "", "F", -1),
]
structureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df2 = spark.createDataFrame(data=structureData, schema=structureSchema)
df2.printSchema()
df2.show(truncate=False)


updatedDF = df2.withColumn(
    "OtherInfo",
    struct(
        col("id").alias("identifier"),
        col("gender").alias("gender"),
        col("salary").alias("salary"),
        when(col("salary").cast(IntegerType()) < 2000, "Low")
        .when(col("salary").cast(IntegerType()) < 4000, "Medium")
        .otherwise("High")
        .alias("Salary_Grade"),
    ),
).drop("id", "gender", "salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)


""" Array & Map"""


arrayStructureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("hobbies", ArrayType(StringType()), True),
        StructField("properties", MapType(StringType(), StringType()), True),
    ]
)

"""# **Column Class | Operators & Functions**

**Create Column Class Object**
"""

from pyspark.sql.functions import lit

colObj = lit("sparkbyexamples.com")

data = [("James", 23), ("Ann", 40)]
df = spark.createDataFrame(data).toDF("name", "gender")
df.printSchema()

# Using DataFrame object (df)
df.select(df.gender).show()
df.select(df["gender"]).show()
# Accessing column name with dot (with backticks)
df.select(df["`name`"]).show()

# Using SQL col() function
from pyspark.sql.functions import col

df.select(col("gender")).show()
# Accessing column name with dot (with backticks)
df.select(col("`name`")).show()

data = [
    ("James", "Bond", "100", None),
    ("Ann", "Varsa", "200", "F"),
    ("Tom Cruise", "XXX", "400", ""),
    ("Tom Brand", None, "400", "M"),
]
columns = ["fname", "lname", "id", "gender"]
df = spark.createDataFrame(data, columns)
df.show()

# alias
from pyspark.sql.functions import expr

df.select(df.fname.alias("first_name"), df.lname.alias("last_name")).show()

# Another example
df.select(expr(" fname ||','|| lname").alias("fullName")).show()

df.sort(df.fname.asc()).show()
df.sort(df.fname.desc()).show()

df.filter(df.id.between(100, 300)).show()

df.filter(df.fname.contains("B")).show()

df.select(df.fname, df.lname, df.id).filter(df.fname.like("%om"))

from pyspark.sql.functions import when

df.select(
    df.fname,
    df.lname,
    when(df.gender == "M", "Male")
    .when(df.gender == "F", "Female")
    .when(df.gender == None, "")
    .otherwise(df.gender)
    .alias("new_gender"),
).show()

df.filter(df.gender.isNull()).show()

df.printSchema()

# getItem() used with ArrayType
# df.select(df.languages.getItem(1)).show()

# getItem() used with MapType
df.select(df.lname.getItem("xxx")).show()


"""# **Select Columns**"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Spark.examples.com").getOrCreate()
data = [
    ("mahesh ihfweoifow", "sci", 15, "ssoa"),
    ("ramesh", "comm", 17, "xaviers"),
    ("fenil a;h;hwo;eihfwoeih", "arts", 16, "dhl"),
    ("kenil", "sci", 18, "infocity"),
]
columns = ["Name", "stream", "Age", "School"]
df = spark.createDataFrame(data=data, schema=columns)
df.show()
df.printSchema()

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()
data = [
    ("James", "Smith", "USA", "CA"),
    ("Michael", "Rose", "USA", "NY"),
    ("Robert", "Williams", "USA", "CA"),
    ("Maria", "Jones", "USA", "FL"),
]
columns = ["firstname", "lastname", "country", "state"]
df = spark.createDataFrame(data=data, schema=columns)
df.show(truncate=False)

df.select("firstname", "lastname").show()
df.select(df.firstname, df.lastname).show()
df.select(df["firstname"], df["lastname"]).show()

# By using col() function
from pyspark.sql.functions import col

df.select(col("firstname"), col("lastname")).show()

# Select columns by regular expression
df.select(df.colRegex("`^.*name*`")).show()

df.select(*columns).show()

# Select All columns
df.select([col for col in df.columns]).show()
df.select("*").show()

# Selects first 3 columns and top 3 rows
df.select(df.columns[:3]).show(3)

# Selects columns 2 to 4  and top 3 r
df.select(df.columns[2:4]).show(3)


"""# **COLLECT()**

**collect() is an action hence it does not return a DataFrame instead, it returns data in an Array to the driver.**
"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
deptColumns = ["dept_name", "dept_id"]
deptDF = spark.createDataFrame(data=dept, schema=deptColumns)
deptDF.show(truncate=False)

dataCollect = deptDF.collect()
print(dataCollect)

for row in dataCollect:
    print(row["dept_name"] + "," + str(row["dept_id"]))

"""# **Where Filter**"""

from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import StringType, IntegerType, ArrayType

data = [
    (("James", "", "Smith"), ["Java", "Scala", "C++"], "OH", "M"),
    (("Anna", "Rose", ""), ["Spark", "Java", "C++"], "NY", "F"),
    (("Julia", "", "Williams"), ["CSharp", "VB"], "OH", "F"),
    (("Maria", "Anne", "Jones"), ["CSharp", "VB"], "NY", "M"),
    (("Jen", "Mary", "Brown"), ["CSharp", "VB"], "NY", "M"),
    (("Mike", "Mary", "Williams"), ["Python", "VB"], "OH", "M"),
]

schema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("languages", ArrayType(StringType()), True),
        StructField("state", StringType(), True),
        StructField("gender", StringType(), True),
    ]
)

df = spark.createDataFrame(data=data, schema=schema)
df.printSchema()
df.show(truncate=False)

df.filter(df.state == "OH").show(truncate=False)

# not equals condition
df.filter(df.state != "OH").show(truncate=False)
df.filter(~(df.state == "OH")).show(truncate=False)

df.filter("gender <> 'M'").show()

li = ["OH", "CA", "DE"]
df.filter(df.state.isin(li)).show()

# Filter NOT IS IN List values
# These show all records with NY (NY is not part of the list)
df.filter(~df.state.isin(li)).show()
df.filter(df.state.isin(li) == False).show()

# Prepare Data
data2 = [
    (2, "Michael Rose"),
    (3, "Robert Williams"),
    (4, "Rames Rose"),
    (5, "Rames rose"),
]
df2 = spark.createDataFrame(data=data2, schema=["id", "name"])

# like - SQL LIKE pattern
df2.filter(df2.name.like("%rose%")).show()

df2.filter(df2.name.rlike("(?i)^*rose$")).show()

"""# **Distinct to Drop Duplicate**

distinct() transformation is used to drop/remove the duplicate rows (all columns) from DataFrame and dropDuplicates() is used to drop rows based on selected (one or multiple) columns.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

data = [
    ("James", "Sales", 3000),
    ("Michael", "Sales", 4600),
    ("Robert", "Sales", 4100),
    ("Maria", "Finance", 3000),
    ("James", "Sales", 3000),
    ("Scott", "Finance", 3300),
    ("Jen", "Finance", 3900),
    ("Jeff", "Marketing", 3000),
    ("Kumar", "Marketing", 2000),
    ("Saif", "Sales", 4100),
]
columns = ["employee_name", "department", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()
df.show(truncate=False)

# Distinct
distinctDF = df.distinct()
print("Distinct count: " + str(distinctDF.count()))
distinctDF.show(truncate=False)

# Drop duplicates
df2 = df.dropDuplicates()
print("Distinct count: " + str(df2.count()))
df2.show(truncate=False)

# Drop duplicates on selected columns
dropDisDF = df.dropDuplicates(["department", "salary"])
print("Distinct count of department salary : " + str(dropDisDF.count()))
dropDisDF.show(truncate=False)

"""# **MAP()**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

data = [
    "Project",
    "Gutenberg’s",
    "Alice’s",
    "Adventures",
    "in",
    "Wonderland",
    "Project",
    "Gutenberg’s",
    "Adventures",
    "in",
    "Wonderland",
    "Project",
    "Gutenberg’s",
]

rdd = spark.sparkContext.parallelize(data)

rdd2 = rdd.map(lambda x: (x, 1))
for element in rdd2.collect():
    print(element)

data = [
    ("James", "Smith", "M", 30),
    ("Anna", "Rose", "F", 41),
    ("Robert", "Williams", "M", 62),
]

columns = ["firstname", "lastname", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.show()

rdd2 = df.rdd.map(lambda x: (x[0] + "," + x[1], x[2], x[3] * 2))
df2 = rdd2.toDF(["name", "gender", "new_salary"])
df2.show()

# Referring Column Names
rdd2 = df.rdd.map(
    lambda x: (x["firstname"] + "," + x["lastname"], x["gender"], x["salary"] * 2)
)

# Referring Column Names
rdd2 = df.rdd.map(lambda x: (x.firstname + "," + x.lastname, x.gender, x.salary * 2))


def func1(x):
    firstName = x.firstname
    lastName = x.lastname
    name = firstName + "," + lastName
    gender = x.gender.lower()
    salary = x.salary * 2
    return (name, gender, salary)


rdd2 = df.rdd.map(lambda x: func1(x))

"""# **FLAT MAP()**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

data = [
    "Project Gutenberg’s",
    "Alice’s Adventures in Wonderland",
    "Project Gutenberg’s",
    "Adventures in Wonderland",
    "Project Gutenberg’s",
]
rdd = spark.sparkContext.parallelize(data)
for element in rdd.collect():
    print(element)

# Flatmap
rdd2 = rdd.flatMap(lambda x: x.split(" "))
for element in rdd2.collect():
    print(element)

"""**Unfortunately, PySpark DataFame doesn’t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column.**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("pyspark-by-examples").getOrCreate()

arrayData = [
    ("James", ["Java", "Scala"], {"hair": "black", "eye": "brown"}),
    ("Michael", ["Spark", "Java", None], {"hair": "brown", "eye": None}),
    ("Robert", ["CSharp", ""], {"hair": "red", "eye": ""}),
    ("Washington", None, None),
    ("Jefferson", ["1", "2"], {}),
]
df = spark.createDataFrame(
    data=arrayData, schema=["name", "knownLanguages", "properties"]
)

from pyspark.sql.functions import explode

df2 = df.select(df.name, explode(df.knownLanguages))
df2.printSchema()
df2.show()

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

df = spark.range(100)
print(df.sample(0.05, 123).collect())
