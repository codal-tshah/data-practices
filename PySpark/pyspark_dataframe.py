# -*- coding: utf-8 -*-
"""PySpark DataFrame.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlaE1lpnh3TCh0Tr4_0Os-ttIL5OUizs
"""

# !pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkExample.com").getOrCreate()

"""# **Create an empty DataFrame**"""

emptyRDD = spark.sparkContext.emptyRDD()
print(emptyRDD)

"""**Create Empty DataFrame with Schema (StructType)**"""

from pyspark.sql.types import StructType, StructField, StringType

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
    ]
)

df = spark.createDataFrame(emptyRDD, schema)
df.printSchema()
df.show()

"""**Convert Empty RDD to DataFrame**"""

df2 = spark.createDataFrame([], schema)
df2.printSchema()
df2.show()

"""# **Convert PySpark RDD to DataFrame**

**Create PySpark RDD**
"""

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

"""**Completed Example**"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()

dept = [("Finance", 10), ("Marketing", 20), ("Sales", 30), ("IT", 40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name", "dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema=deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

from pyspark.sql.types import StructType, StructField, StringType

deptSchema = StructType(
    [
        StructField("dept_name", StringType(), True),
        StructField("dept_id", StringType(), True),
    ]
)

deptDF1 = spark.createDataFrame(rdd, schema=deptSchema)
deptDF1.printSchema()
deptDF1.show(truncate=False)

"""SHOW()"""

df2.show()

df2.show(2, truncate=False)

df2.show(2, truncate=3)

df2.show(2, truncate=False, vertical=True)

print(df2.schema.json())


"""# **StructType & StructField**"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    ArrayType,
    MapType,
)
from pyspark.sql.functions import col, struct, when

spark = (
    SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
)

data = [
    ("James", "", "Smith", "36636", "M", 3000),
    ("Michael", "Rose", "", "40288", "M", 4000),
    ("Robert", "", "Williams", "42114", "M", 4000),
    ("Maria", "Anne", "Jones", "39192", "F", 4000),
    ("Jen", "Mary", "Brown", "", "F", -1),
]

schema = StructType(
    [
        StructField("firstname", StringType(), True),
        StructField("middlename", StringType(), True),
        StructField("lastname", StringType(), True),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df = spark.createDataFrame(data=data, schema=schema)
df.printSchema()
df.show(truncate=False)

structureData = [
    (("James", "", "Smith"), "36636", "M", 3100),
    (("Michael", "Rose", ""), "40288", "M", 4300),
    (("Robert", "", "Williams"), "42114", "M", 1400),
    (("Maria", "Anne", "Jones"), "39192", "F", 5500),
    (("Jen", "Mary", "Brown"), "", "F", -1),
]
structureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("id", StringType(), True),
        StructField("gender", StringType(), True),
        StructField("salary", IntegerType(), True),
    ]
)

df2 = spark.createDataFrame(data=structureData, schema=structureSchema)
df2.printSchema()
df2.show(truncate=False)


updatedDF = df2.withColumn(
    "OtherInfo",
    struct(
        col("id").alias("identifier"),
        col("gender").alias("gender"),
        col("salary").alias("salary"),
        when(col("salary").cast(IntegerType()) < 2000, "Low")
        .when(col("salary").cast(IntegerType()) < 4000, "Medium")
        .otherwise("High")
        .alias("Salary_Grade"),
    ),
).drop("id", "gender", "salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)


""" Array & Map"""


arrayStructureSchema = StructType(
    [
        StructField(
            "name",
            StructType(
                [
                    StructField("firstname", StringType(), True),
                    StructField("middlename", StringType(), True),
                    StructField("lastname", StringType(), True),
                ]
            ),
        ),
        StructField("hobbies", ArrayType(StringType()), True),
        StructField("properties", MapType(StringType(), StringType()), True),
    ]
)

"""# **Column Class | Operators & Functions**

**Create Column Class Object**
"""

from pyspark.sql.functions import lit

colObj = lit("sparkbyexamples.com")

data = [("James", 23), ("Ann", 40)]
df = spark.createDataFrame(data).toDF("name", "gender")
df.printSchema()

# Using DataFrame object (df)
df.select(df.gender).show()
df.select(df["gender"]).show()
# Accessing column name with dot (with backticks)
df.select(df["`name`"]).show()

# Using SQL col() function
from pyspark.sql.functions import col

df.select(col("gender")).show()
# Accessing column name with dot (with backticks)
df.select(col("`name`")).show()

data = [
    ("James", "Bond", "100", None),
    ("Ann", "Varsa", "200", "F"),
    ("Tom Cruise", "XXX", "400", ""),
    ("Tom Brand", None, "400", "M"),
]
columns = ["fname", "lname", "id", "gender"]
df = spark.createDataFrame(data, columns)
df.show()

# alias
from pyspark.sql.functions import expr

df.select(df.fname.alias("first_name"), df.lname.alias("last_name")).show()

# Another example
df.select(expr(" fname ||','|| lname").alias("fullName")).show()

df.sort(df.fname.asc()).show()
df.sort(df.fname.desc()).show()

df.filter(df.id.between(100, 300)).show()

df.filter(df.fname.contains("B")).show()

df.select(df.fname, df.lname, df.id).filter(df.fname.like("%om"))

from pyspark.sql.functions import when

df.select(
    df.fname,
    df.lname,
    when(df.gender == "M", "Male")
    .when(df.gender == "F", "Female")
    .when(df.gender == None, "")
    .otherwise(df.gender)
    .alias("new_gender"),
).show()

df.filter(df.gender.isNull()).show()

df.printSchema()

# getItem() used with ArrayType
# df.select(df.languages.getItem(1)).show()

# getItem() used with MapType
df.select(df.lname.getItem("xxx")).show()


"""# **Select Columns**"""

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Spark.examples.com").getOrCreate()
data = [
    ("mahesh ihfweoifow", "sci", 15, "ssoa"),
    ("ramesh", "comm", 17, "xaviers"),
    ("fenil a;h;hwo;eihfwoeih", "arts", 16, "dhl"),
    ("kenil", "sci", 18, "infocity"),
]
columns = ["Name", "stream", "Age", "School"]
df = spark.createDataFrame(data=data, schema=columns)
df.show()
df.printSchema()

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SparkByExamples.com").getOrCreate()
data = [
    ("James", "Smith", "USA", "CA"),
    ("Michael", "Rose", "USA", "NY"),
    ("Robert", "Williams", "USA", "CA"),
    ("Maria", "Jones", "USA", "FL"),
]
columns = ["firstname", "lastname", "country", "state"]
df = spark.createDataFrame(data=data, schema=columns)
df.show(truncate=False)

df.select("firstname", "lastname").show()
df.select(df.firstname, df.lastname).show()
df.select(df["firstname"], df["lastname"]).show()

# By using col() function
from pyspark.sql.functions import col

df.select(col("firstname"), col("lastname")).show()

# Select columns by regular expression
df.select(df.colRegex("`^.*name*`")).show()

df.select(*columns).show()

# Select All columns
df.select([col for col in df.columns]).show()
df.select("*").show()

# Selects first 3 columns and top 3 rows
df.select(df.columns[:3]).show(3)

# Selects columns 2 to 4  and top 3 r
df.select(df.columns[2:4]).show(3)
