13_may:
    - Data Warehouse:
        - Here's a simple example:
                - Let's say you're running an e-commerce website. You collect data on customer transactions, including information like order details, customer demographics, and website interactions. This data is collected from your website's database and various logs.

                - You use an ETL tool like Apache Airflow or AWS Glue to extract this data, transform it into a suitable format (e.g., cleaning, aggregating, joining tables), and then load it into AWS Redshift. Once the data is in Redshift, you can run SQL queries to analyze it. For example, you might want to analyze sales trends, customer behavior, or product performance.

                - Finally, you use a BI tool like Tableau or Amazon QuickSight to visualize the insights gained from the analysis, creating charts and dashboards that help you understand and communicate the data effectively.
        - learned extraction of data and merge it.
        - learning ETL pipelines extraction and tranformation

    - ClaimDeck:
        - Started writing Pytest for numbersviewset
        - writing for claim_valuation function 

14_may:
    - Data Warehouse:
        - Setting up github conflict
        - Ended making of ETL pipelines

    - ClaimDeck:
        - (spent 8 hours on pytest only)
        - Setup setup_model_objects for numbersviewset
        - Written Pytest for numbersviewset
            - test_get_serializer_class_valuation_data
            - test_get_serializer_class_budget_data
            - test_get_serializer_class_policy_data
            - test_get_serializer_class_liens_data
            - test_get_serializer_class_reserve_data
            - test_get_valuation_data
            - test_get_policy_data
            - test_get_liens_data_with_liens
            - test_get_liens_data_with_no_data
            - test_get_liens_invalid_claim_id
        - covered up the 70% for numbersviewset and overall viewset increased upto 18% to 22%            