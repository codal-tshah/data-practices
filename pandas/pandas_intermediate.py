# -*- coding: utf-8 -*-
"""pandas_intermediate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fvheiggiT7IGV1M99yjJBXNSNixpdeWq
"""

from google.colab import drive

drive.mount("/content/drive")

"""# **Create plots in pandas**

"""

import pandas as pd
import matplotlib.pyplot as plt

"""**parse_dates=True, helps to plot the dates properly**"""

data = pd.read_csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2.csv",
    index_col=0,
    parse_dates=True,
)
data.head()

data.plot()
plt.show()

paris = data["station_paris"].plot()
# plt.show()

"""**alpha is use for opacity of points in graph** bold text"""

data.plot.scatter(x="station_london", y="station_paris", alpha=0.5).plot()

data.plot.box()

"""**subplots is use for multiple plots of columns**"""

areas = data.plot.area(figsize=(12, 8), subplots=True)
# areas

data.plot.line(y="station_london")

fig, axs = plt.subplots(figsize=(12, 4))  # Create an empty Matplotlib Figure and Axes
data.plot.area(ax=axs)  # Use pandas to put the area plot on the prepared Figure/Axes
axs.set_ylabel("concentration")  # Do any Matplotlib customization you like
fig.savefig(
    "concentrations.png"
)  # Save the Figure/Axes using the existing Matplotlib method.
plt.show()  # Display the plot

"""# **create new columns derived from existing columns**"""

data

data["london_mg_per_cubic"] = data["station_london"] * 1.882
data

"""**Ratio Paris versus Antwerp**"""

data["Paris_ratio"] = data["station_paris"] / data["station_antwerp"]
data

columns_renamed = data.rename(
    columns={
        "station_antwerp": "Antwerp",
        "station_london": "London",
        "station_paris": "Paris",
    }
)
columns_renamed.head()

"""**converting the column names to lowercase letters can be done using a function**"""

columns_renamed = columns_renamed.rename(columns=str.lower)
columns_renamed.head()

"""# **calculate summary statistics**"""

import pandas as pd

titanic_data = pd.read_csv(
    "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv"
)
titanic_data.head()

age_mean = titanic_data[["Age"]].mean()
print(age_mean)

age_fare_median = titanic_data[["Age", "Fare"]].median()
age_fare_median

titanic_data.agg(
    {
        "Age": ["min", "max", "median", "mean", "skew"],
        "Fare": ["min", "max", "median", "mean", "skew"],
    }
)

"""**Aggregating statistics grouped by category**"""

titanic_data[["Age", "Sex"]].groupby("Sex").mean()

"""**groupby provides the power of the split-apply-combine pattern**"""

titanic_data[["Age", "Embarked"]].groupby("Embarked").mean()

titanic_data.groupby("Sex")["Age"].mean()

titanic_data.groupby(["Sex", "Pclass"])[["Fare"]].mean()

titanic_data.groupby(["Sex", "Pclass"])["Age"].mean()

titanic_data["Pclass"].value_counts()

"""**value_counts() method counts the number of records for each category in a column.**"""

titanic_data["Sex"].value_counts()

"""# **Reshape the layout of tables**"""

titanic_data = pd.read_csv(
    "/content/drive/MyDrive/data_practices/pandas/data/titanic.csv"
)
titanic_data.head()

air_quality_long = pd.read_csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_long.csv",
    index_col="date.utc",
    parse_dates=True,
)
air_quality_long.head()

titanic_data.sort_values("Age").head()

titanic_data.sort_values(["Pclass", "Age"], ascending=False).head()

no2 = air_quality_long[air_quality_long["parameter"] == "no2"]
no2.head()

no2_subset = no2.sort_index().groupby(["location"]).head(2)
no2_subset

no2_subset.pivot(columns="location", values="value")

no2.pivot(columns="location", values="value").plot()

air_quality_long.pivot_table(
    values="value", index="location", columns="parameter", aggfunc="mean"
)

"""**margins (subtotals) for each variable, set the margins parameter to True**"""

air_quality_long.pivot_table(
    values="value",
    index="location",
    columns="parameter",
    aggfunc="mean",
    margins=True,
)

air_quality_long.groupby(["location", "parameter"])[["value"]].mean()

no2_pivoted = no2.pivot(columns="location", values="value").reset_index()
no2_pivoted

no2_single = no2.pivot(columns=["location", "value"])
no2_single

"""**pandas.melt() method on a DataFrame converts the data table from wide format to long format. The column headers become the variable names in a newly created column.**"""

no_2 = no2_pivoted.melt(id_vars="date.utc")
no_2

no_2 = no2_pivoted.melt(
    id_vars="date.utc",
    value_vars=["BETR801", "FR04014", "London Westminster"],
    value_name="NO_2",
    var_name="id_location",
)
no_2

"""# **combine data from multiple tables**"""

import pandas as pd

air_quality_no2 = pd.read_csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2_long.csv",
    parse_dates=True,
)
air_quality_no2 = air_quality_no2[["date.utc", "location", "parameter", "value"]]
air_quality_no2.head()

air_quality_pm25 = pd.read_csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_pm25_long.csv",
    parse_dates=True,
)
air_quality_pm25 = air_quality_pm25[["date.utc", "location", "parameter", "value"]]
air_quality_pm25.head()

"""**The concat() function performs concatenation operations of multiple tables along one of the axes (row-wise or column-wise)**"""

air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
air_quality

print("Shape of the ``air_quality_pm25`` table: ", air_quality_pm25.shape)

print("Shape of the ``air_quality_no2`` table: ", air_quality_no2.shape)

print("Shape of the resulting ``air_quality`` table: ", air_quality.shape)

air_quality = air_quality.sort_values("date.utc")
air_quality

air_quality.reset_index(level=0)

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", "NO2"])
air_quality_

merged_data = pd.merge(
    air_quality_no2,
    air_quality_pm25,
    left_on=["location", "parameter"],
    right_on=["location", "parameter"],
)
merged_data

"""# **Handle time series data**"""

import pandas as pd
import matplotlib.pyplot as plt

air_quality = pd.read_csv(
    "/content/drive/MyDrive/data_practices/pandas/data/air_quality_no2_long.csv"
)
air_quality = air_quality.rename(columns={"date.utc": "datetime"})
air_quality.head()

"""**.unique() is use to showcase number of unique items present in the column**"""

print("locations: ", air_quality.location.unique())
print("cities: ", air_quality.city.unique())

"""**By applying the to_datetime function, pandas interprets the strings and convert these to datetime (i.e. datetime64[ns, UTC]) objects**"""

air_quality["datetime"] = pd.to_datetime(air_quality["datetime"])
air_quality["datetime"]

"""*2nd approach to convert dates:

```
# pd.read_csv("air_quality_no2_long.csv", parse_dates=["datetime"])
```

**start and end date of data**
"""

air_quality["datetime"].min(), air_quality["datetime"].max()

"""**Using pandas.Timestamp for datetimes enables us to calculate with date information and make them comparable**"""

air_quality["datetime"].max() - air_quality["datetime"].min()

air_quality["month"] = air_quality["datetime"].dt.month
air_quality.head()

air_quality.groupby([air_quality["datetime"].dt.weekday, "location"])["value"].mean()

air_quality["datetime"].dt.weekday
air_quality

fig, axs = plt.subplots(figsize=(11, 4))
air_quality.groupby(air_quality["datetime"].dt.hour)["value"].mean().plot(
    kind="bar", rot=0, ax=axs
)
plt.xlabel("Hour of the day")  # custom x label using Matplotlib
plt.ylabel("$NO_2 (µg/m^3)$")

"""**Datetime as index**"""

no_2 = air_quality.pivot(index="datetime", columns="location", values="value")
no_2.head()

no_2.index.year, no_2.index.weekday

no_2["2019-05-20":"2019-05-21"].plot()

"""**Aggregate the current hourly time series values to the weekly maximum value in each of the stations**"""

weekly_max = no_2.resample("W").max()
weekly_max

"""*The resample() method is similar to a groupby operation:

it provides a time-based grouping, by using a string (e.g. M, 5H,…) that defines the target frequency

it requires an aggregation function such as mean, max*
"""

monthly_mean = no_2.resample("M").mean()
monthly_mean

"""**The frequency of the time series is provided by the freq attribute:**"""

weekly_max.index.freq

monthly_mean.index.freq

"""**Make a plot of the daily mean NO2 value in each of the station**"""

no_2.resample("D").mean().plot(style="-o", figsize=(11, 5))

"""**plotted max value per day**"""

no_2.resample("D").max().plot(style="-^", figsize=(11, 5))

no_2.resample("W").max().plot(style="-^", figsize=(11, 5))

"""# **Manipulate textual data**"""

import pandas as pd

titanic = pd.read_csv("/content/drive/MyDrive/data_practices/pandas/data/titanic.csv")
titanic.head()

"""**Make all name characters lowercase.**

**add the str accessor **
"""

titanic["Name"].str.lower()

titanic["Name"].str.upper()

titanic["Surname"] = titanic["Name"].str.split(",")
titanic.head()

titanic["Surname"]

"""** Series.str.get() to extract the relevant part**"""

titanic["Surname"] = titanic["Name"].str.split(",").str.get(0)
titanic.head()

"""**Series.str.contains() checks for each of the values in the column Name if the string contains the word Countess and returns for each of the values True**"""

titanic[titanic["Name"].str.contains("Countess")]

titanic[titanic["Name"].str.contains("Mrs")]

titanic["Surname"].value_counts()

titanic[titanic["Surname"].str.contains("Andersson")]

"""**passenger of the Titanic has the longest name?**

The idxmax() method does exactly that. It is not a string method and is applied to integers
"""

print("id:", titanic["Name"].str.len().idxmax())
print("max len:", titanic["Name"].str.len().max())
print("name:", titanic.loc[307, "Name"])

titanic.loc[307, "Name"]

passenger_308_name = titanic.loc[titanic["PassengerId"] == 308, "Name"].values[0]
passenger_308_name

titanic.loc[titanic["Name"].str.len().idxmax(), "Name"]

titanic["Sex_shorts"] = titanic["Sex"].replace({"male": "M", "female": "F"})
titanic.head()

titanic["Sex"].str.replace("female", "F")
